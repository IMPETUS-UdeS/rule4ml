{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Optionally force tensorflow on CPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "1. [General Usage](#general-usage)\n",
    "    1. [Basic Usage](#basic-usage)\n",
    "    2. [Advanced Usage](#advanced-usage)\n",
    "2. [Data Generation](#data-gen)\n",
    "    1. [Model Synthesis](#model-synth)\n",
    "        1. [Keras Synthesis](#keras-synth)\n",
    "        2. [PyTorch Synthesis](#torch-synth)\n",
    "    2. [Parallel Synthesis](#parallel-synth)\n",
    "        1. [Randomly Generated Networks](#random-synth)\n",
    "3. [Training Prediction Models](#train-models)\n",
    "    1. [Parsing Datasets](#parse-data)\n",
    "        1. [Reading from JSON](#read-json)\n",
    "    2. [Training MLPs](#train-mlps)\n",
    "        1. [Data Preprocessing](#mlp-data)\n",
    "        2. [Building & Training](#fit-mlps)\n",
    "    3. [Finetuning (Optional)](#finetune)\n",
    "        1. [Finetuning an MLP](#finetune-mlp)\n",
    "        2. [Loading and Retraining](#load-tuner)\n",
    "4. [Testing Prediction Models](#test-models)\n",
    "    1. [Benchmark Networks](#benchmark-test)\n",
    "    2. [Plots](#plots)\n",
    "        1. [Box Plots](#box-plots)\n",
    "        2. [Bar Plots](#bar-plots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. General Usage <a class=\"anchor\" id=\"general-usage\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Basic Usage <a class=\"anchor\" id=\"basic-usage\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Activation\n",
    "import keras\n",
    "\n",
    "# Example of a keras Model to predict\n",
    "input_size = 16\n",
    "inputs = Input(shape=(input_size,))\n",
    "x = Dense(32, activation=\"relu\")(inputs)\n",
    "x = Dense(32, activation=\"relu\")(x)\n",
    "x = Dense(32, activation=\"relu\")(x)\n",
    "outputs = Dense(5, activation=\"softmax\")(x)\n",
    "\n",
    "model_to_predict = keras.Model(inputs=inputs, outputs=outputs, name=\"Jet Classifier\")\n",
    "model_to_predict.build((None, input_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rule4ml.models.estimators import MultiModelEstimator\n",
    "\n",
    "# Load default estimator\n",
    "estimator = MultiModelEstimator()\n",
    "estimator.load_default_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MultiModelEstimator predictions are formatted as a DataFrame\n",
    "prediction_df = estimator.predict(model_to_predict)\n",
    "\n",
    "# each row is unique in the groupby, mean() is only called to convert DataFrameGroupBy into a nicely organized DataFrame\n",
    "if not prediction_df.empty:\n",
    "    prediction_df = prediction_df.groupby(\n",
    "        [\"Model\", \"Board\", \"Strategy\", \"Precision\", \"Reuse Factor\"], observed=True\n",
    "    ).mean()\n",
    "\n",
    "prediction_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df.to_html(\"keras_example.html\")\n",
    "\n",
    "# prediction_df.to_latex(\"keras_example.tex\")\n",
    "# prediction_df.to_csv(\"keras_example.csv\")\n",
    "# prediction_df.to_json(\"keras_example.json\")\n",
    "# prediction_df.to_xml(\"keras_example.xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Advanced Usage <a class=\"anchor\" id=\"advanced-usage\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "import keras\n",
    "from keras.layers import Input, Dense\n",
    "import torch\n",
    "\n",
    "models_to_predict = []\n",
    "\n",
    "\n",
    "# Example of a subclassed PyTorch model\n",
    "class MyTopQuarks(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyTopQuarks, self).__init__()\n",
    "\n",
    "        self.dense1 = torch.nn.Linear(10, 32)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.dense2 = torch.nn.Linear(32, 1)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.dense1(inputs)\n",
    "        x = self.relu(x)\n",
    "        x = self.dense2(x)\n",
    "        outputs = self.sigmoid(x)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "models_to_predict.append(MyTopQuarks())\n",
    "\n",
    "# Example of a keras Sequential model\n",
    "input_size = 16\n",
    "model_to_predict = keras.Sequential(\n",
    "    layers=[\n",
    "        keras.layers.Input(shape=(input_size,)),\n",
    "        keras.layers.Dense(32, use_bias=True),\n",
    "        keras.layers.Activation(\"relu\"),\n",
    "        keras.layers.Dense(32, use_bias=True),\n",
    "        keras.layers.Activation(\"relu\"),\n",
    "        keras.layers.Dense(32, use_bias=True),\n",
    "        keras.layers.Activation(\"relu\"),\n",
    "        keras.layers.Dense(5, use_bias=True),\n",
    "        keras.layers.Activation(\"softmax\"),\n",
    "    ],\n",
    "    name=\"Jet Classifier\",\n",
    ")\n",
    "model_to_predict.build((None, input_size))\n",
    "\n",
    "models_to_predict.append(model_to_predict)\n",
    "\n",
    "hls_configs = [\n",
    "    {\n",
    "        \"model\": {\n",
    "            \"precision\": \"ap_fixed<8, 3>\",\n",
    "            \"reuse_factor\": 32,\n",
    "            \"strategy\": strategy,\n",
    "        },\n",
    "        \"board\": board,\n",
    "    }\n",
    "    for board, strategy in itertools.product([\"pynq-z2\", \"zcu102\"], [\"Latency\", \"Resource\"])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rule4ml.models.estimators import ModelWrapper\n",
    "\n",
    "lut_model_wrapper = ModelWrapper()\n",
    "lut_model_wrapper.load(\"./models/best_LUT_MLP_config.json\", \"./models/best_LUT_MLP.weights.h5\")\n",
    "\n",
    "lut_model_wrapper.predict(models_to_predict, hls_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rule4ml.models.estimators import MultiModelEstimator\n",
    "\n",
    "estimator = MultiModelEstimator()\n",
    "estimator.add_model_wrapper(lut_model_wrapper)\n",
    "\n",
    "estimator.predict(models_to_predict, hls_configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Generation <a class=\"anchor\" id=\"data-gen\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Specify Vivado path\n",
    "os.environ[\"PATH\"] = \"/opt/Xilinx/Vivado/2019.1/bin:\" + os.environ[\"PATH\"]\n",
    "\n",
    "base_path = os.path.join(os.getcwd(), \"..\", \"data_gen\")\n",
    "sys.path.append(base_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Model Synthesis <a class=\"anchor\" id=\"model-synth\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Keras Model <a class=\"anchor\" id=\"keras-synth\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Input, Dense\n",
    "\n",
    "from data_gen.nn_synth import synthesize_keras_model\n",
    "\n",
    "input_size = 16\n",
    "inputs = Input(shape=(input_size,))\n",
    "x = Dense(32, activation=\"relu\")(inputs)\n",
    "x = Dense(32, activation=\"relu\")(x)\n",
    "x = Dense(32, activation=\"relu\")(x)\n",
    "outputs = Dense(5, activation=\"softmax\")(x)\n",
    "\n",
    "model_to_synthesize = keras.Model(inputs=inputs, outputs=outputs, name=\"Jet Classifier\")\n",
    "model_to_synthesize.build((None, input_size))\n",
    "\n",
    "synthesis_result = synthesize_keras_model(\n",
    "    model_to_synthesize,\n",
    "    board=\"pynq-z2\",\n",
    "    strategy=\"Resource\",\n",
    "    precision=\"ap_fixed<8, 3>\",\n",
    "    reuse_factor=32,\n",
    "    clock_period=\"10\",\n",
    "    io_type=\"io_parallel\",\n",
    "    project_dir=\"./hls4ml_prj\",\n",
    "    synth_uuid=None,\n",
    "    verbose=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_gen.utils import save_to_json\n",
    "\n",
    "save_to_json(synthesis_result, \"./synthesis_result.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 PyTorch Model <a class=\"anchor\" id=\"torch-synth\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from data_gen.nn_synth import synthesize_torch_model\n",
    "from data_gen.utils import save_to_json\n",
    "\n",
    "model_to_synthesize = torch.nn.Sequential(\n",
    "    torch.nn.Linear(10, 32),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(32, 1),\n",
    "    torch.nn.Sigmoid(),\n",
    ")\n",
    "\n",
    "synthesis_result = synthesize_torch_model(\n",
    "    model_to_synthesize,\n",
    "    board=\"zcu102\",\n",
    "    strategy=\"Latency\",\n",
    "    precision=\"ap_fixed<8, 3>\",\n",
    "    reuse_factor=32,\n",
    "    clock_period=\"10\",\n",
    "    io_type=\"io_parallel\",\n",
    "    project_dir=\"./hls4ml_prj\",\n",
    "    synth_uuid=None,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "save_to_json(synthesis_result, \"./synthesis_result.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Parallel Synthesis <a class=\"anchor\" id=\"parallel-synth\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Randomly Generated Networks <a class=\"anchor\" id=\"random-synth\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "import time\n",
    "\n",
    "from data_gen.nn_gen import GeneratorSettings, generate_fc_network\n",
    "from data_gen.nn_synth import (\n",
    "    SynthSettings,\n",
    "    synthesize_keras_model,\n",
    "    parallel_generative_synthesis,\n",
    ")\n",
    "\n",
    "from data_gen.utils import IntRange, Power2Range, save_to_json\n",
    "\n",
    "gen_settings = GeneratorSettings(\n",
    "    input_range=Power2Range(16, 32),\n",
    "    layer_range=IntRange(2, 3),\n",
    "    neuron_range=Power2Range(16, 32),\n",
    "    output_range=IntRange(1, 20),\n",
    "    activations=[\"relu\"],\n",
    ")\n",
    "synth_settings = SynthSettings(\n",
    "    reuse_range=Power2Range(32, 64),\n",
    "    precisions=[\"ap_fixed<2, 1>\", \"ap_fixed<8, 3>\"],\n",
    "    strategies=[\"Resource\"],\n",
    ")\n",
    "\n",
    "n_procs = 3\n",
    "with Pool(n_procs) as p:\n",
    "    result = p.map_async(\n",
    "        parallel_generative_synthesis,\n",
    "        [\n",
    "            {\n",
    "                \"job_id\": f\"{proc}\",\n",
    "                \"n_models\": 10,\n",
    "                \"project_dir\": \"./projects\",\n",
    "                \"prj_overwrite\": False,\n",
    "                \"save_path\": \"./\",\n",
    "                \"rng_seed\": 0,\n",
    "                \"gen_function\": generate_fc_network,  # Keras only currently\n",
    "                \"gen_settings\": gen_settings,\n",
    "                \"synth_function\": synthesize_keras_model,\n",
    "                \"synth_settings\": synth_settings,\n",
    "            }\n",
    "            for proc in range(1, n_procs + 1)\n",
    "        ],\n",
    "    )\n",
    "    while not result.ready():\n",
    "        time.sleep(1)\n",
    "    result = result.get()\n",
    "    p.terminate()\n",
    "    p.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training Prediction Models <a class=\"anchor\" id=\"train-models\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Parsing Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 Reading from JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rule4ml.parsers.data_parser import (\n",
    "    read_from_json,\n",
    "    get_global_data,\n",
    "    get_sequential_data,\n",
    "    to_dataframe,\n",
    ")\n",
    "\n",
    "from rule4ml.parsers.data_parser import (\n",
    "    default_board_map,\n",
    "    default_strategy_map,\n",
    "    default_layer_type_map,\n",
    ")\n",
    "\n",
    "# data_filter = ParsedDataFilter(\n",
    "# max_output_size=200,\n",
    "# )\n",
    "\n",
    "base_path = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "train_json_data = read_from_json(\n",
    "    os.path.join(base_path, \"datasets\", \"wa-hls4ml_train_split.json\"),\n",
    "    # data_filter,\n",
    ")\n",
    "test_json_data = read_from_json(\n",
    "    os.path.join(base_path, \"datasets\", \"wa-hls4ml_test_split.json\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataframe: (90313, 141)\n",
      "Test Dataframe: (22579, 141)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uuid</th>\n",
       "      <th>model_name</th>\n",
       "      <th>artifacts_file</th>\n",
       "      <th>strategy</th>\n",
       "      <th>board</th>\n",
       "      <th>model_precision</th>\n",
       "      <th>model_total_bits</th>\n",
       "      <th>model_integer_bits</th>\n",
       "      <th>model_fractional_bits</th>\n",
       "      <th>global_reuse</th>\n",
       "      <th>reuse_mean</th>\n",
       "      <th>weight_total_bits</th>\n",
       "      <th>weight_integer_bits</th>\n",
       "      <th>weight_fractional_bits</th>\n",
       "      <th>dense_inputs_mean</th>\n",
       "      <th>dense_inputs_min</th>\n",
       "      <th>dense_inputs_min_idx</th>\n",
       "      <th>dense_inputs_max</th>\n",
       "      <th>dense_inputs_max_idx</th>\n",
       "      <th>dense_outputs_mean</th>\n",
       "      <th>dense_outputs_min</th>\n",
       "      <th>dense_outputs_min_idx</th>\n",
       "      <th>dense_outputs_max</th>\n",
       "      <th>dense_outputs_max_idx</th>\n",
       "      <th>dense_parameters_mean</th>\n",
       "      <th>dense_parameters_min</th>\n",
       "      <th>dense_parameters_min_idx</th>\n",
       "      <th>dense_parameters_max</th>\n",
       "      <th>dense_parameters_max_idx</th>\n",
       "      <th>dense_reuse_mean</th>\n",
       "      <th>dense_reuse_min</th>\n",
       "      <th>dense_reuse_min_idx</th>\n",
       "      <th>dense_reuse_max</th>\n",
       "      <th>dense_reuse_max_idx</th>\n",
       "      <th>dense_count</th>\n",
       "      <th>batchnormalization_inputs_mean</th>\n",
       "      <th>batchnormalization_inputs_min</th>\n",
       "      <th>batchnormalization_inputs_min_idx</th>\n",
       "      <th>batchnormalization_inputs_max</th>\n",
       "      <th>batchnormalization_inputs_max_idx</th>\n",
       "      <th>batchnormalization_outputs_mean</th>\n",
       "      <th>batchnormalization_outputs_min</th>\n",
       "      <th>batchnormalization_outputs_min_idx</th>\n",
       "      <th>batchnormalization_outputs_max</th>\n",
       "      <th>batchnormalization_outputs_max_idx</th>\n",
       "      <th>batchnormalization_parameters_mean</th>\n",
       "      <th>batchnormalization_parameters_min</th>\n",
       "      <th>batchnormalization_parameters_min_idx</th>\n",
       "      <th>batchnormalization_parameters_max</th>\n",
       "      <th>batchnormalization_parameters_max_idx</th>\n",
       "      <th>batchnormalization_count</th>\n",
       "      <th>add_inputs_mean</th>\n",
       "      <th>add_inputs_min</th>\n",
       "      <th>add_inputs_min_idx</th>\n",
       "      <th>add_inputs_max</th>\n",
       "      <th>add_inputs_max_idx</th>\n",
       "      <th>add_outputs_mean</th>\n",
       "      <th>add_outputs_min</th>\n",
       "      <th>add_outputs_min_idx</th>\n",
       "      <th>add_outputs_max</th>\n",
       "      <th>add_outputs_max_idx</th>\n",
       "      <th>add_count</th>\n",
       "      <th>concatenate_inputs_mean</th>\n",
       "      <th>concatenate_inputs_min</th>\n",
       "      <th>concatenate_inputs_min_idx</th>\n",
       "      <th>concatenate_inputs_max</th>\n",
       "      <th>concatenate_inputs_max_idx</th>\n",
       "      <th>concatenate_outputs_mean</th>\n",
       "      <th>concatenate_outputs_min</th>\n",
       "      <th>concatenate_outputs_min_idx</th>\n",
       "      <th>concatenate_outputs_max</th>\n",
       "      <th>concatenate_outputs_max_idx</th>\n",
       "      <th>concatenate_count</th>\n",
       "      <th>dropout_inputs_mean</th>\n",
       "      <th>dropout_inputs_min</th>\n",
       "      <th>dropout_inputs_min_idx</th>\n",
       "      <th>dropout_inputs_max</th>\n",
       "      <th>dropout_inputs_max_idx</th>\n",
       "      <th>dropout_outputs_mean</th>\n",
       "      <th>dropout_outputs_min</th>\n",
       "      <th>dropout_outputs_min_idx</th>\n",
       "      <th>dropout_outputs_max</th>\n",
       "      <th>dropout_outputs_max_idx</th>\n",
       "      <th>dropout_count</th>\n",
       "      <th>relu_inputs_mean</th>\n",
       "      <th>relu_inputs_min</th>\n",
       "      <th>relu_inputs_min_idx</th>\n",
       "      <th>relu_inputs_max</th>\n",
       "      <th>relu_inputs_max_idx</th>\n",
       "      <th>relu_outputs_mean</th>\n",
       "      <th>relu_outputs_min</th>\n",
       "      <th>relu_outputs_min_idx</th>\n",
       "      <th>relu_outputs_max</th>\n",
       "      <th>relu_outputs_max_idx</th>\n",
       "      <th>relu_count</th>\n",
       "      <th>sigmoid_inputs_mean</th>\n",
       "      <th>sigmoid_inputs_min</th>\n",
       "      <th>sigmoid_inputs_min_idx</th>\n",
       "      <th>sigmoid_inputs_max</th>\n",
       "      <th>sigmoid_inputs_max_idx</th>\n",
       "      <th>sigmoid_outputs_mean</th>\n",
       "      <th>sigmoid_outputs_min</th>\n",
       "      <th>sigmoid_outputs_min_idx</th>\n",
       "      <th>sigmoid_outputs_max</th>\n",
       "      <th>sigmoid_outputs_max_idx</th>\n",
       "      <th>sigmoid_count</th>\n",
       "      <th>tanh_inputs_mean</th>\n",
       "      <th>tanh_inputs_min</th>\n",
       "      <th>tanh_inputs_min_idx</th>\n",
       "      <th>tanh_inputs_max</th>\n",
       "      <th>tanh_inputs_max_idx</th>\n",
       "      <th>tanh_outputs_mean</th>\n",
       "      <th>tanh_outputs_min</th>\n",
       "      <th>tanh_outputs_min_idx</th>\n",
       "      <th>tanh_outputs_max</th>\n",
       "      <th>tanh_outputs_max_idx</th>\n",
       "      <th>tanh_count</th>\n",
       "      <th>softmax_inputs_mean</th>\n",
       "      <th>softmax_inputs_min</th>\n",
       "      <th>softmax_inputs_min_idx</th>\n",
       "      <th>softmax_inputs_max</th>\n",
       "      <th>softmax_inputs_max_idx</th>\n",
       "      <th>softmax_outputs_mean</th>\n",
       "      <th>softmax_outputs_min</th>\n",
       "      <th>softmax_outputs_min_idx</th>\n",
       "      <th>softmax_outputs_max</th>\n",
       "      <th>softmax_outputs_max_idx</th>\n",
       "      <th>softmax_count</th>\n",
       "      <th>total_mult</th>\n",
       "      <th>total_add</th>\n",
       "      <th>total_logical</th>\n",
       "      <th>total_lookup</th>\n",
       "      <th>clock_period</th>\n",
       "      <th>hls4ml_version</th>\n",
       "      <th>vivado_version</th>\n",
       "      <th>sequential_inputs</th>\n",
       "      <th>bram</th>\n",
       "      <th>dsp</th>\n",
       "      <th>ff</th>\n",
       "      <th>lut</th>\n",
       "      <th>cycles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>494ebda9-bf32-4ef4-ab88-47f1bd42e8e2</td>\n",
       "      <td>dense_88_56_112_6b</td>\n",
       "      <td>494ebda9-bf32-4ef4-ab88-47f1bd42e8e2.tar.gz</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>ap_fixed&lt;16,6&gt;</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>3070</td>\n",
       "      <td>3070.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>56</td>\n",
       "      <td>3</td>\n",
       "      <td>88</td>\n",
       "      <td>1</td>\n",
       "      <td>84.0</td>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>112</td>\n",
       "      <td>3</td>\n",
       "      <td>5684.0</td>\n",
       "      <td>4984</td>\n",
       "      <td>1</td>\n",
       "      <td>6384</td>\n",
       "      <td>3</td>\n",
       "      <td>3070.0</td>\n",
       "      <td>3070</td>\n",
       "      <td>1</td>\n",
       "      <td>3070</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>181888</td>\n",
       "      <td>179552</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.8.1</td>\n",
       "      <td>2020.1</td>\n",
       "      <td>layer_type  layer_input_size  layer_output_...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8633.0</td>\n",
       "      <td>7210.0</td>\n",
       "      <td>5604.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e12136c3-e6ab-46cc-b580-c61df52fd715</td>\n",
       "      <td>dense_120_120_16_14b</td>\n",
       "      <td>e12136c3-e6ab-46cc-b580-c61df52fd715.tar.gz</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>ap_fixed&lt;16,6&gt;</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>3070</td>\n",
       "      <td>3070.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>120</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>1</td>\n",
       "      <td>68.0</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>120</td>\n",
       "      <td>1</td>\n",
       "      <td>8228.0</td>\n",
       "      <td>1936</td>\n",
       "      <td>3</td>\n",
       "      <td>14520</td>\n",
       "      <td>1</td>\n",
       "      <td>3070.0</td>\n",
       "      <td>3070</td>\n",
       "      <td>1</td>\n",
       "      <td>3070</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>263296</td>\n",
       "      <td>259424</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.8.1</td>\n",
       "      <td>2020.1</td>\n",
       "      <td>layer_type  layer_input_size  layer_output_...</td>\n",
       "      <td>8.5</td>\n",
       "      <td>6.0</td>\n",
       "      <td>14793.0</td>\n",
       "      <td>10267.0</td>\n",
       "      <td>4804.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4dc5410c-baa9-4c43-97f0-20c11c48950b</td>\n",
       "      <td>dense_112_96_120_12b</td>\n",
       "      <td>4dc5410c-baa9-4c43-97f0-20c11c48950b.tar.gz</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>ap_fixed&lt;16,6&gt;</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>2047</td>\n",
       "      <td>2047.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>96</td>\n",
       "      <td>3</td>\n",
       "      <td>112</td>\n",
       "      <td>1</td>\n",
       "      <td>108.0</td>\n",
       "      <td>96</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>3</td>\n",
       "      <td>11244.0</td>\n",
       "      <td>10848</td>\n",
       "      <td>1</td>\n",
       "      <td>11640</td>\n",
       "      <td>3</td>\n",
       "      <td>2047.0</td>\n",
       "      <td>2047</td>\n",
       "      <td>1</td>\n",
       "      <td>2047</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>359808</td>\n",
       "      <td>356448</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.8.1</td>\n",
       "      <td>2020.1</td>\n",
       "      <td>layer_type  layer_input_size  layer_output_...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>13383.0</td>\n",
       "      <td>10306.0</td>\n",
       "      <td>3716.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2c47746d-2a0c-4b04-8c8a-27cc6b64aae1</td>\n",
       "      <td>dense_24_56_96_16b</td>\n",
       "      <td>2c47746d-2a0c-4b04-8c8a-27cc6b64aae1.tar.gz</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>ap_fixed&lt;16,6&gt;</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>2047</td>\n",
       "      <td>2047.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>56</td>\n",
       "      <td>3</td>\n",
       "      <td>76.0</td>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>96</td>\n",
       "      <td>3</td>\n",
       "      <td>3436.0</td>\n",
       "      <td>1400</td>\n",
       "      <td>1</td>\n",
       "      <td>5472</td>\n",
       "      <td>3</td>\n",
       "      <td>2047.0</td>\n",
       "      <td>2047</td>\n",
       "      <td>1</td>\n",
       "      <td>2047</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>109952</td>\n",
       "      <td>108640</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.8.1</td>\n",
       "      <td>2020.1</td>\n",
       "      <td>layer_type  layer_input_size  layer_output_...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9194.0</td>\n",
       "      <td>6415.0</td>\n",
       "      <td>3140.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>f814561e-e6f1-4b54-bcac-bf3df399e5f7</td>\n",
       "      <td>dense_96_48_64_8b</td>\n",
       "      <td>f814561e-e6f1-4b54-bcac-bf3df399e5f7.tar.gz</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>ap_fixed&lt;16,6&gt;</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>48</td>\n",
       "      <td>3</td>\n",
       "      <td>96</td>\n",
       "      <td>1</td>\n",
       "      <td>56.0</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>3</td>\n",
       "      <td>3896.0</td>\n",
       "      <td>3136</td>\n",
       "      <td>3</td>\n",
       "      <td>4656</td>\n",
       "      <td>1</td>\n",
       "      <td>1024.0</td>\n",
       "      <td>1024</td>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>124672</td>\n",
       "      <td>122336</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.8.1</td>\n",
       "      <td>2020.1</td>\n",
       "      <td>layer_type  layer_input_size  layer_output_...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6846.0</td>\n",
       "      <td>6132.0</td>\n",
       "      <td>1924.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   uuid            model_name  \\\n",
       "0  494ebda9-bf32-4ef4-ab88-47f1bd42e8e2    dense_88_56_112_6b   \n",
       "1  e12136c3-e6ab-46cc-b580-c61df52fd715  dense_120_120_16_14b   \n",
       "2  4dc5410c-baa9-4c43-97f0-20c11c48950b  dense_112_96_120_12b   \n",
       "3  2c47746d-2a0c-4b04-8c8a-27cc6b64aae1    dense_24_56_96_16b   \n",
       "4  f814561e-e6f1-4b54-bcac-bf3df399e5f7     dense_96_48_64_8b   \n",
       "\n",
       "                                artifacts_file  strategy  board  \\\n",
       "0  494ebda9-bf32-4ef4-ab88-47f1bd42e8e2.tar.gz         2      4   \n",
       "1  e12136c3-e6ab-46cc-b580-c61df52fd715.tar.gz         2      4   \n",
       "2  4dc5410c-baa9-4c43-97f0-20c11c48950b.tar.gz         2      4   \n",
       "3  2c47746d-2a0c-4b04-8c8a-27cc6b64aae1.tar.gz         2      4   \n",
       "4  f814561e-e6f1-4b54-bcac-bf3df399e5f7.tar.gz         2      4   \n",
       "\n",
       "  model_precision  model_total_bits  model_integer_bits  \\\n",
       "0  ap_fixed<16,6>                16                   6   \n",
       "1  ap_fixed<16,6>                16                   6   \n",
       "2  ap_fixed<16,6>                16                   6   \n",
       "3  ap_fixed<16,6>                16                   6   \n",
       "4  ap_fixed<16,6>                16                   6   \n",
       "\n",
       "   model_fractional_bits  global_reuse  reuse_mean  weight_total_bits  \\\n",
       "0                     10          3070      3070.0                6.0   \n",
       "1                     10          3070      3070.0               14.0   \n",
       "2                     10          2047      2047.0               12.0   \n",
       "3                     10          2047      2047.0               16.0   \n",
       "4                     10          1024      1024.0                8.0   \n",
       "\n",
       "   weight_integer_bits  weight_fractional_bits  dense_inputs_mean  \\\n",
       "0                  1.0                     5.0               72.0   \n",
       "1                  1.0                    13.0              120.0   \n",
       "2                  1.0                    11.0              104.0   \n",
       "3                  1.0                    15.0               40.0   \n",
       "4                  1.0                     7.0               72.0   \n",
       "\n",
       "   dense_inputs_min  dense_inputs_min_idx  dense_inputs_max  \\\n",
       "0                56                     3                88   \n",
       "1               120                     1               120   \n",
       "2                96                     3               112   \n",
       "3                24                     1                56   \n",
       "4                48                     3                96   \n",
       "\n",
       "   dense_inputs_max_idx  dense_outputs_mean  dense_outputs_min  \\\n",
       "0                     1                84.0                 56   \n",
       "1                     1                68.0                 16   \n",
       "2                     1               108.0                 96   \n",
       "3                     3                76.0                 56   \n",
       "4                     1                56.0                 48   \n",
       "\n",
       "   dense_outputs_min_idx  dense_outputs_max  dense_outputs_max_idx  \\\n",
       "0                      1                112                      3   \n",
       "1                      3                120                      1   \n",
       "2                      1                120                      3   \n",
       "3                      1                 96                      3   \n",
       "4                      1                 64                      3   \n",
       "\n",
       "   dense_parameters_mean  dense_parameters_min  dense_parameters_min_idx  \\\n",
       "0                 5684.0                  4984                         1   \n",
       "1                 8228.0                  1936                         3   \n",
       "2                11244.0                 10848                         1   \n",
       "3                 3436.0                  1400                         1   \n",
       "4                 3896.0                  3136                         3   \n",
       "\n",
       "   dense_parameters_max  dense_parameters_max_idx  dense_reuse_mean  \\\n",
       "0                  6384                         3            3070.0   \n",
       "1                 14520                         1            3070.0   \n",
       "2                 11640                         3            2047.0   \n",
       "3                  5472                         3            2047.0   \n",
       "4                  4656                         1            1024.0   \n",
       "\n",
       "   dense_reuse_min  dense_reuse_min_idx  dense_reuse_max  dense_reuse_max_idx  \\\n",
       "0             3070                    1             3070                    1   \n",
       "1             3070                    1             3070                    1   \n",
       "2             2047                    1             2047                    1   \n",
       "3             2047                    1             2047                    1   \n",
       "4             1024                    1             1024                    1   \n",
       "\n",
       "   dense_count  batchnormalization_inputs_mean  batchnormalization_inputs_min  \\\n",
       "0            2                               0                              0   \n",
       "1            2                               0                              0   \n",
       "2            2                               0                              0   \n",
       "3            2                               0                              0   \n",
       "4            2                               0                              0   \n",
       "\n",
       "   batchnormalization_inputs_min_idx  batchnormalization_inputs_max  \\\n",
       "0                                  0                              0   \n",
       "1                                  0                              0   \n",
       "2                                  0                              0   \n",
       "3                                  0                              0   \n",
       "4                                  0                              0   \n",
       "\n",
       "   batchnormalization_inputs_max_idx  batchnormalization_outputs_mean  \\\n",
       "0                                  0                                0   \n",
       "1                                  0                                0   \n",
       "2                                  0                                0   \n",
       "3                                  0                                0   \n",
       "4                                  0                                0   \n",
       "\n",
       "   batchnormalization_outputs_min  batchnormalization_outputs_min_idx  \\\n",
       "0                               0                                   0   \n",
       "1                               0                                   0   \n",
       "2                               0                                   0   \n",
       "3                               0                                   0   \n",
       "4                               0                                   0   \n",
       "\n",
       "   batchnormalization_outputs_max  batchnormalization_outputs_max_idx  \\\n",
       "0                               0                                   0   \n",
       "1                               0                                   0   \n",
       "2                               0                                   0   \n",
       "3                               0                                   0   \n",
       "4                               0                                   0   \n",
       "\n",
       "   batchnormalization_parameters_mean  batchnormalization_parameters_min  \\\n",
       "0                                   0                                  0   \n",
       "1                                   0                                  0   \n",
       "2                                   0                                  0   \n",
       "3                                   0                                  0   \n",
       "4                                   0                                  0   \n",
       "\n",
       "   batchnormalization_parameters_min_idx  batchnormalization_parameters_max  \\\n",
       "0                                      0                                  0   \n",
       "1                                      0                                  0   \n",
       "2                                      0                                  0   \n",
       "3                                      0                                  0   \n",
       "4                                      0                                  0   \n",
       "\n",
       "   batchnormalization_parameters_max_idx  batchnormalization_count  \\\n",
       "0                                      0                         0   \n",
       "1                                      0                         0   \n",
       "2                                      0                         0   \n",
       "3                                      0                         0   \n",
       "4                                      0                         0   \n",
       "\n",
       "   add_inputs_mean  add_inputs_min  add_inputs_min_idx  add_inputs_max  \\\n",
       "0                0               0                   0               0   \n",
       "1                0               0                   0               0   \n",
       "2                0               0                   0               0   \n",
       "3                0               0                   0               0   \n",
       "4                0               0                   0               0   \n",
       "\n",
       "   add_inputs_max_idx  add_outputs_mean  add_outputs_min  add_outputs_min_idx  \\\n",
       "0                   0                 0                0                    0   \n",
       "1                   0                 0                0                    0   \n",
       "2                   0                 0                0                    0   \n",
       "3                   0                 0                0                    0   \n",
       "4                   0                 0                0                    0   \n",
       "\n",
       "   add_outputs_max  add_outputs_max_idx  add_count  concatenate_inputs_mean  \\\n",
       "0                0                    0          0                        0   \n",
       "1                0                    0          0                        0   \n",
       "2                0                    0          0                        0   \n",
       "3                0                    0          0                        0   \n",
       "4                0                    0          0                        0   \n",
       "\n",
       "   concatenate_inputs_min  concatenate_inputs_min_idx  concatenate_inputs_max  \\\n",
       "0                       0                           0                       0   \n",
       "1                       0                           0                       0   \n",
       "2                       0                           0                       0   \n",
       "3                       0                           0                       0   \n",
       "4                       0                           0                       0   \n",
       "\n",
       "   concatenate_inputs_max_idx  concatenate_outputs_mean  \\\n",
       "0                           0                         0   \n",
       "1                           0                         0   \n",
       "2                           0                         0   \n",
       "3                           0                         0   \n",
       "4                           0                         0   \n",
       "\n",
       "   concatenate_outputs_min  concatenate_outputs_min_idx  \\\n",
       "0                        0                            0   \n",
       "1                        0                            0   \n",
       "2                        0                            0   \n",
       "3                        0                            0   \n",
       "4                        0                            0   \n",
       "\n",
       "   concatenate_outputs_max  concatenate_outputs_max_idx  concatenate_count  \\\n",
       "0                        0                            0                  0   \n",
       "1                        0                            0                  0   \n",
       "2                        0                            0                  0   \n",
       "3                        0                            0                  0   \n",
       "4                        0                            0                  0   \n",
       "\n",
       "   dropout_inputs_mean  dropout_inputs_min  dropout_inputs_min_idx  \\\n",
       "0                    0                   0                       0   \n",
       "1                    0                   0                       0   \n",
       "2                    0                   0                       0   \n",
       "3                    0                   0                       0   \n",
       "4                    0                   0                       0   \n",
       "\n",
       "   dropout_inputs_max  dropout_inputs_max_idx  dropout_outputs_mean  \\\n",
       "0                   0                       0                     0   \n",
       "1                   0                       0                     0   \n",
       "2                   0                       0                     0   \n",
       "3                   0                       0                     0   \n",
       "4                   0                       0                     0   \n",
       "\n",
       "   dropout_outputs_min  dropout_outputs_min_idx  dropout_outputs_max  \\\n",
       "0                    0                        0                    0   \n",
       "1                    0                        0                    0   \n",
       "2                    0                        0                    0   \n",
       "3                    0                        0                    0   \n",
       "4                    0                        0                    0   \n",
       "\n",
       "   dropout_outputs_max_idx  dropout_count  relu_inputs_mean  relu_inputs_min  \\\n",
       "0                        0              0                 0                0   \n",
       "1                        0              0                 0                0   \n",
       "2                        0              0                 0                0   \n",
       "3                        0              0                 0                0   \n",
       "4                        0              0                 0                0   \n",
       "\n",
       "   relu_inputs_min_idx  relu_inputs_max  relu_inputs_max_idx  \\\n",
       "0                    0                0                    0   \n",
       "1                    0                0                    0   \n",
       "2                    0                0                    0   \n",
       "3                    0                0                    0   \n",
       "4                    0                0                    0   \n",
       "\n",
       "   relu_outputs_mean  relu_outputs_min  relu_outputs_min_idx  \\\n",
       "0                  0                 0                     0   \n",
       "1                  0                 0                     0   \n",
       "2                  0                 0                     0   \n",
       "3                  0                 0                     0   \n",
       "4                  0                 0                     0   \n",
       "\n",
       "   relu_outputs_max  relu_outputs_max_idx  relu_count  sigmoid_inputs_mean  \\\n",
       "0                 0                     0           0                    0   \n",
       "1                 0                     0           0                    0   \n",
       "2                 0                     0           0                    0   \n",
       "3                 0                     0           0                    0   \n",
       "4                 0                     0           0                    0   \n",
       "\n",
       "   sigmoid_inputs_min  sigmoid_inputs_min_idx  sigmoid_inputs_max  \\\n",
       "0                   0                       0                   0   \n",
       "1                   0                       0                   0   \n",
       "2                   0                       0                   0   \n",
       "3                   0                       0                   0   \n",
       "4                   0                       0                   0   \n",
       "\n",
       "   sigmoid_inputs_max_idx  sigmoid_outputs_mean  sigmoid_outputs_min  \\\n",
       "0                       0                     0                    0   \n",
       "1                       0                     0                    0   \n",
       "2                       0                     0                    0   \n",
       "3                       0                     0                    0   \n",
       "4                       0                     0                    0   \n",
       "\n",
       "   sigmoid_outputs_min_idx  sigmoid_outputs_max  sigmoid_outputs_max_idx  \\\n",
       "0                        0                    0                        0   \n",
       "1                        0                    0                        0   \n",
       "2                        0                    0                        0   \n",
       "3                        0                    0                        0   \n",
       "4                        0                    0                        0   \n",
       "\n",
       "   sigmoid_count  tanh_inputs_mean  tanh_inputs_min  tanh_inputs_min_idx  \\\n",
       "0              0                 0                0                    0   \n",
       "1              0                 0                0                    0   \n",
       "2              0                 0                0                    0   \n",
       "3              0                 0                0                    0   \n",
       "4              0                 0                0                    0   \n",
       "\n",
       "   tanh_inputs_max  tanh_inputs_max_idx  tanh_outputs_mean  tanh_outputs_min  \\\n",
       "0                0                    0                  0                 0   \n",
       "1                0                    0                  0                 0   \n",
       "2                0                    0                  0                 0   \n",
       "3                0                    0                  0                 0   \n",
       "4                0                    0                  0                 0   \n",
       "\n",
       "   tanh_outputs_min_idx  tanh_outputs_max  tanh_outputs_max_idx  tanh_count  \\\n",
       "0                     0                 0                     0           0   \n",
       "1                     0                 0                     0           0   \n",
       "2                     0                 0                     0           0   \n",
       "3                     0                 0                     0           0   \n",
       "4                     0                 0                     0           0   \n",
       "\n",
       "   softmax_inputs_mean  softmax_inputs_min  softmax_inputs_min_idx  \\\n",
       "0                    0                   0                       0   \n",
       "1                    0                   0                       0   \n",
       "2                    0                   0                       0   \n",
       "3                    0                   0                       0   \n",
       "4                    0                   0                       0   \n",
       "\n",
       "   softmax_inputs_max  softmax_inputs_max_idx  softmax_outputs_mean  \\\n",
       "0                   0                       0                     0   \n",
       "1                   0                       0                     0   \n",
       "2                   0                       0                     0   \n",
       "3                   0                       0                     0   \n",
       "4                   0                       0                     0   \n",
       "\n",
       "   softmax_outputs_min  softmax_outputs_min_idx  softmax_outputs_max  \\\n",
       "0                    0                        0                    0   \n",
       "1                    0                        0                    0   \n",
       "2                    0                        0                    0   \n",
       "3                    0                        0                    0   \n",
       "4                    0                        0                    0   \n",
       "\n",
       "   softmax_outputs_max_idx  softmax_count  total_mult  total_add  \\\n",
       "0                        0              0      181888     179552   \n",
       "1                        0              0      263296     259424   \n",
       "2                        0              0      359808     356448   \n",
       "3                        0              0      109952     108640   \n",
       "4                        0              0      124672     122336   \n",
       "\n",
       "   total_logical  total_lookup  clock_period hls4ml_version vivado_version  \\\n",
       "0              0             0           5.0          0.8.1         2020.1   \n",
       "1              0             0           5.0          0.8.1         2020.1   \n",
       "2              0             0           5.0          0.8.1         2020.1   \n",
       "3              0             0           5.0          0.8.1         2020.1   \n",
       "4              0             0           5.0          0.8.1         2020.1   \n",
       "\n",
       "                                   sequential_inputs  bram   dsp       ff  \\\n",
       "0     layer_type  layer_input_size  layer_output_...   4.0   0.0   8633.0   \n",
       "1     layer_type  layer_input_size  layer_output_...   8.5   6.0  14793.0   \n",
       "2     layer_type  layer_input_size  layer_output_...   9.0  12.0  13383.0   \n",
       "3     layer_type  layer_input_size  layer_output_...   4.0   4.0   9194.0   \n",
       "4     layer_type  layer_input_size  layer_output_...   3.5   8.0   6846.0   \n",
       "\n",
       "       lut  cycles  \n",
       "0   7210.0  5604.0  \n",
       "1  10267.0  4804.0  \n",
       "2  10306.0  3716.0  \n",
       "3   6415.0  3140.0  \n",
       "4   6132.0  1924.0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_meta_data, train_global_inputs, train_targets = get_global_data(train_json_data)\n",
    "train_sequential_inputs = get_sequential_data(train_json_data)\n",
    "\n",
    "test_meta_data, test_global_inputs, test_targets = get_global_data(test_json_data)\n",
    "test_sequential_inputs = get_sequential_data(test_json_data)\n",
    "\n",
    "# Ordinal encoding of categorical inputs\n",
    "global_categorical_maps = {\n",
    "    \"strategy\": default_strategy_map,\n",
    "    \"board\": default_board_map,\n",
    "}\n",
    "sequential_categorical_maps = {\n",
    "    \"layer_type\": default_layer_type_map,\n",
    "}\n",
    "\n",
    "train_df = to_dataframe(\n",
    "    meta_data=train_meta_data,\n",
    "    global_inputs=train_global_inputs,\n",
    "    sequential_inputs=train_sequential_inputs,\n",
    "    global_categorical_maps=global_categorical_maps,\n",
    "    sequential_categorical_maps=sequential_categorical_maps,\n",
    "    targets=train_targets,\n",
    ")\n",
    "\n",
    "test_df = to_dataframe(\n",
    "    meta_data=test_meta_data,\n",
    "    global_inputs=test_global_inputs,\n",
    "    sequential_inputs=test_sequential_inputs,\n",
    "    global_categorical_maps=global_categorical_maps,\n",
    "    sequential_categorical_maps=sequential_categorical_maps,\n",
    "    targets=test_targets,\n",
    ")\n",
    "\n",
    "print(f\"Train Dataframe: {train_df.shape}\")\n",
    "print(f\"Test Dataframe: {test_df.shape}\")\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os, json\n",
    "\n",
    "# def save_to_json(data, file_path=\"./dataset.json\", indent=2):\n",
    "#     \"\"\"\n",
    "#     _summary_\n",
    "\n",
    "#     Args:\n",
    "#         data (_type_): _description_\n",
    "#         file_path (str, optional): _description_. Defaults to './dataset.json'.\n",
    "#         indent (int, optional): _description_. Defaults to 2.\n",
    "#     \"\"\"\n",
    "\n",
    "#     if not os.path.isfile(file_path):  # Create file if it doesn't exist\n",
    "#         with open(file_path, \"w\") as json_file:\n",
    "#             json.dump([data], json_file, indent=indent)\n",
    "#     else:  # If it exists then just append to the file\n",
    "#         with open(file_path, \"r+\") as json_file:\n",
    "#             json_file.seek(os.stat(file_path).st_size - 2)\n",
    "#             json_file.write(\n",
    "#                 \",\\n\"\n",
    "#                 + \" \" * indent\n",
    "#                 + \"{}\\n]\".format(json.dumps(data, indent=indent).replace(\"\\n\", \"\\n\" + \" \" * indent))\n",
    "#             )\n",
    "\n",
    "# # split and save a json file using the uuids in the test dataframe\n",
    "# for data in json_data:\n",
    "#     if data[\"meta_data\"][\"uuid\"] in train_df[\"uuid\"].values:\n",
    "#         save_to_json(data, f\"./wa-hls4ml_train_split.json\")\n",
    "#     if data[\"meta_data\"][\"uuid\"] in test_df[\"uuid\"].values:\n",
    "#         save_to_json(data, f\"./wa-hls4ml_test_split.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Training MLPs <a class=\"anchor\" id=\"train-mlps\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Data Preprocessing <a class=\"anchor\" id=\"mlp-data\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>strategy</th>\n",
       "      <th>board</th>\n",
       "      <th>clock_period</th>\n",
       "      <th>reuse_mean</th>\n",
       "      <th>weight_total_bits</th>\n",
       "      <th>dense_inputs_mean</th>\n",
       "      <th>dense_outputs_mean</th>\n",
       "      <th>dense_parameters_mean</th>\n",
       "      <th>dense_reuse_mean</th>\n",
       "      <th>dense_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3070.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>5684.0</td>\n",
       "      <td>3070.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3070.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>8228.0</td>\n",
       "      <td>3070.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2047.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>11244.0</td>\n",
       "      <td>2047.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2047.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>3436.0</td>\n",
       "      <td>2047.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1024.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>3896.0</td>\n",
       "      <td>1024.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   strategy  board  clock_period  reuse_mean  weight_total_bits  \\\n",
       "0         2      4           5.0      3070.0                6.0   \n",
       "1         2      4           5.0      3070.0               14.0   \n",
       "2         2      4           5.0      2047.0               12.0   \n",
       "3         2      4           5.0      2047.0               16.0   \n",
       "4         2      4           5.0      1024.0                8.0   \n",
       "\n",
       "   dense_inputs_mean  dense_outputs_mean  dense_parameters_mean  \\\n",
       "0               72.0                84.0                 5684.0   \n",
       "1              120.0                68.0                 8228.0   \n",
       "2              104.0               108.0                11244.0   \n",
       "3               40.0                76.0                 3436.0   \n",
       "4               72.0                56.0                 3896.0   \n",
       "\n",
       "   dense_reuse_mean  dense_count  \n",
       "0            3070.0            2  \n",
       "1            3070.0            2  \n",
       "2            2047.0            2  \n",
       "3            2047.0            2  \n",
       "4            1024.0            2  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_labels = [  # Selecting input features\n",
    "    \"strategy\",\n",
    "    \"board\",\n",
    "    \"clock_period\",\n",
    "    # \"precision\",\n",
    "    # \"model_total_bits\",\n",
    "    # \"model_integer_bits\",\n",
    "    # \"model_fractional_bits\",\n",
    "    \"reuse_mean\",\n",
    "    \"weight_total_bits\",\n",
    "    # \"weight_integer_bits\",\n",
    "    # \"weight_fractional_bits\",\n",
    "    # \"dense_count\",\n",
    "    # \"batchnormalization_count\",\n",
    "    # \"add_count\",\n",
    "    # \"concatenate_count\",\n",
    "    # \"dropout_count\",\n",
    "    # \"relu_count\",\n",
    "    # \"sigmoid_count\",\n",
    "    # \"tanh_count\",\n",
    "    # \"softmax_count\",\n",
    "    # \"dense_parameters_mean\",\n",
    "    # \"dense_inputs_mean\",\n",
    "    # \"dense_outputs_mean\",\n",
    "    # \"dense_reuse_mean\",\n",
    "    \"dense_inputs_mean\",\n",
    "    # \"dense_inputs_min\",\n",
    "    # \"dense_inputs_min_idx\",\n",
    "    # \"dense_inputs_max\",\n",
    "    # \"dense_inputs_max_idx\",\n",
    "    \"dense_outputs_mean\",\n",
    "    # \"dense_outputs_min\",\n",
    "    # \"dense_outputs_min_idx\",\n",
    "    # \"dense_outputs_max\",\n",
    "    # \"dense_outputs_max_idx\",\n",
    "    \"dense_parameters_mean\",\n",
    "    # \"dense_parameters_min\",\n",
    "    # \"dense_parameters_min_idx\",\n",
    "    # \"dense_parameters_max\",\n",
    "    # \"dense_parameters_max_idx\",\n",
    "    \"dense_reuse_mean\",\n",
    "    # \"dense_reuse_min\",\n",
    "    # \"dense_reuse_min_idx\",\n",
    "    # \"dense_reuse_max\",\n",
    "    # \"dense_reuse_max_idx\",\n",
    "    \"dense_count\",\n",
    "    # \"batchnormalization_inputs_mean\",\n",
    "    # \"batchnormalization_inputs_min\",\n",
    "    # \"batchnormalization_inputs_min_idx\",\n",
    "    # \"batchnormalization_inputs_max\",\n",
    "    # \"batchnormalization_inputs_max_idx\",\n",
    "    # \"batchnormalization_outputs_mean\",\n",
    "    # \"batchnormalization_outputs_min\",\n",
    "    # \"batchnormalization_outputs_min_idx\",\n",
    "    # \"batchnormalization_outputs_max\",\n",
    "    # \"batchnormalization_outputs_max_idx\",\n",
    "    # \"batchnormalization_parameters_mean\",\n",
    "    # \"batchnormalization_parameters_min\",\n",
    "    # \"batchnormalization_parameters_min_idx\",\n",
    "    # \"batchnormalization_parameters_max\",\n",
    "    # \"batchnormalization_parameters_max_idx\",\n",
    "    # \"batchnormalization_count\",\n",
    "    # \"add_inputs_mean\",\n",
    "    # \"add_inputs_min\",\n",
    "    # \"add_inputs_min_idx\",\n",
    "    # \"add_inputs_max\",\n",
    "    # \"add_inputs_max_idx\",\n",
    "    # \"add_outputs_mean\",\n",
    "    # \"add_outputs_min\",\n",
    "    # \"add_outputs_min_idx\",\n",
    "    # \"add_outputs_max\",\n",
    "    # \"add_outputs_max_idx\",\n",
    "    # \"add_count\",\n",
    "    # \"concatenate_inputs_mean\",\n",
    "    # \"concatenate_inputs_min\",\n",
    "    # \"concatenate_inputs_min_idx\",\n",
    "    # \"concatenate_inputs_max\",\n",
    "    # \"concatenate_inputs_max_idx\",\n",
    "    # \"concatenate_outputs_mean\",\n",
    "    # \"concatenate_outputs_min\",\n",
    "    # \"concatenate_outputs_min_idx\",\n",
    "    # \"concatenate_outputs_max\",\n",
    "    # \"concatenate_outputs_max_idx\",\n",
    "    # \"concatenate_count\",\n",
    "    # \"dropout_inputs_mean\",\n",
    "    # \"dropout_inputs_min\",\n",
    "    # \"dropout_inputs_min_idx\",\n",
    "    # \"dropout_inputs_max\",\n",
    "    # \"dropout_inputs_max_idx\",\n",
    "    # \"dropout_outputs_mean\",\n",
    "    # \"dropout_outputs_min\",\n",
    "    # \"dropout_outputs_min_idx\",\n",
    "    # \"dropout_outputs_max\",\n",
    "    # \"dropout_outputs_max_idx\",\n",
    "    # \"dropout_count\",\n",
    "    # \"relu_inputs_mean\",\n",
    "    # \"relu_inputs_min\",\n",
    "    # \"relu_inputs_min_idx\",\n",
    "    # \"relu_inputs_max\",\n",
    "    # \"relu_inputs_max_idx\",\n",
    "    # \"relu_outputs_mean\",\n",
    "    # \"relu_outputs_min\",\n",
    "    # \"relu_outputs_min_idx\",\n",
    "    # \"relu_outputs_max\",\n",
    "    # \"relu_outputs_max_idx\",\n",
    "    # \"relu_count\",\n",
    "    # \"sigmoid_inputs_mean\",\n",
    "    # \"sigmoid_inputs_min\",\n",
    "    # \"sigmoid_inputs_min_idx\",\n",
    "    # \"sigmoid_inputs_max\",\n",
    "    # \"sigmoid_inputs_max_idx\",\n",
    "    # \"sigmoid_outputs_mean\",\n",
    "    # \"sigmoid_outputs_min\",\n",
    "    # \"sigmoid_outputs_min_idx\",\n",
    "    # \"sigmoid_outputs_max\",\n",
    "    # \"sigmoid_outputs_max_idx\",\n",
    "    # \"sigmoid_count\",\n",
    "    # \"tanh_inputs_mean\",\n",
    "    # \"tanh_inputs_min\",\n",
    "    # \"tanh_inputs_min_idx\",\n",
    "    # \"tanh_inputs_max\",\n",
    "    # \"tanh_inputs_max_idx\",\n",
    "    # \"tanh_outputs_mean\",\n",
    "    # \"tanh_outputs_min\",\n",
    "    # \"tanh_outputs_min_idx\",\n",
    "    # \"tanh_outputs_max\",\n",
    "    # \"tanh_outputs_max_idx\",\n",
    "    # \"tanh_count\",\n",
    "    # \"softmax_inputs_mean\",\n",
    "    # \"softmax_inputs_min\",\n",
    "    # \"softmax_inputs_min_idx\",\n",
    "    # \"softmax_inputs_max\",\n",
    "    # \"softmax_inputs_max_idx\",\n",
    "    # \"softmax_outputs_mean\",\n",
    "    # \"softmax_outputs_min\",\n",
    "    # \"softmax_outputs_min_idx\",\n",
    "    # \"softmax_outputs_max\",\n",
    "    # \"softmax_outputs_max_idx\",\n",
    "    # \"softmax_count\",\n",
    "    # \"total_mult\",\n",
    "    # \"total_add\",\n",
    "    # \"total_logical\",\n",
    "    # \"total_lookup\",\n",
    "]\n",
    "\n",
    "train_inputs_df = train_df[feature_labels].copy()\n",
    "test_inputs_df = test_df[feature_labels].copy()\n",
    "\n",
    "train_inputs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lut</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7210.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10267.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10306.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6415.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6132.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       lut\n",
       "0   7210.0\n",
       "1  10267.0\n",
       "2  10306.0\n",
       "3   6415.0\n",
       "4   6132.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_labels = [\"lut\"]\n",
    "\n",
    "train_targets_df = train_df[target_labels].copy()\n",
    "test_targets_df = test_df[target_labels].copy()\n",
    "\n",
    "train_targets_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Building and Training <a class=\"anchor\" id=\"fit-mlps\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-06 10:23:50.365180: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-06 10:23:50.365215: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-06 10:23:50.366144: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-06 10:23:50.371535: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-06 10:23:50.841048: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"LUT_MLP\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " g0_strategy_input (InputLa  [(None, 1)]                  0         []                            \n",
      " yer)                                                                                             \n",
      "                                                                                                  \n",
      " g1_board_input (InputLayer  [(None, 1)]                  0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " strategy_embedding (Embedd  (None, 1, 8)                 24        ['g0_strategy_input[0][0]']   \n",
      " ing)                                                                                             \n",
      "                                                                                                  \n",
      " board_embedding (Embedding  (None, 1, 16)                80        ['g1_board_input[0][0]']      \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " g2_numerical_inputs (Input  [(None, 8)]                  0         []                            \n",
      " Layer)                                                                                           \n",
      "                                                                                                  \n",
      " tf.compat.v1.squeeze (TFOp  (None, 8)                    0         ['strategy_embedding[0][0]']  \n",
      " Lambda)                                                                                          \n",
      "                                                                                                  \n",
      " tf.compat.v1.squeeze_1 (TF  (None, 16)                   0         ['board_embedding[0][0]']     \n",
      " OpLambda)                                                                                        \n",
      "                                                                                                  \n",
      " numerical_dense_0 (Dense)   (None, 256)                  2048      ['g2_numerical_inputs[0][0]'] \n",
      "                                                                                                  \n",
      " tf.concat (TFOpLambda)      (None, 280)                  0         ['tf.compat.v1.squeeze[0][0]',\n",
      "                                                                     'tf.compat.v1.squeeze_1[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'numerical_dense_0[0][0]']   \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 64)                   17984     ['tf.concat[0][0]']           \n",
      "                                                                                                  \n",
      " dropout (Dropout)           (None, 64)                   0         ['dense[0][0]']               \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 32)                   2080      ['dropout[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)         (None, 32)                   0         ['dense_1[0][0]']             \n",
      "                                                                                                  \n",
      " dense_2 (Dense)             (None, 16)                   528       ['dropout_1[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)         (None, 16)                   0         ['dense_2[0][0]']             \n",
      "                                                                                                  \n",
      " dense_3 (Dense)             (None, 32)                   544       ['dropout_2[0][0]']           \n",
      "                                                                                                  \n",
      " dense_4 (Dense)             (None, 32)                   1056      ['dense_3[0][0]']             \n",
      "                                                                                                  \n",
      " dense_5 (Dense)             (None, 1)                    33        ['dense_4[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 24377 (95.22 KB)\n",
      "Trainable params: 24377 (95.22 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-06 10:23:51.247934: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:274] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2024-10-06 10:23:51.247971: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:129] retrieving CUDA diagnostic information for host: 3it-ia-s001.3it.usherbrooke.ca\n",
      "2024-10-06 10:23:51.247976: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:136] hostname: 3it-ia-s001.3it.usherbrooke.ca\n",
      "2024-10-06 10:23:51.248012: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:159] libcuda reported version is: 550.90.7\n",
      "2024-10-06 10:23:51.248026: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:163] kernel reported version is: 550.90.7\n",
      "2024-10-06 10:23:51.248030: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:241] kernel version seems to match DSO: 550.90.7\n"
     ]
    }
   ],
   "source": [
    "from rule4ml.models.estimators import MLPSettings, ModelWrapper\n",
    "\n",
    "input_shape = (None, len(train_inputs_df.columns))\n",
    "output_shape = (None, len(train_targets_df.columns))\n",
    "\n",
    "bram_mlp_settings = MLPSettings(\n",
    "    embedding_outputs=[16 for _ in range(len(global_categorical_maps))],\n",
    "    numerical_dense_layers=[32, 16, 32],\n",
    "    dense_layers=[256, 256, 256, 64, 32, 64, 64],\n",
    "    dense_dropouts=[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    ")\n",
    "dsp_mlp_settings = MLPSettings(\n",
    "    embedding_outputs=[16 for _ in range(len(global_categorical_maps))],\n",
    "    numerical_dense_layers=[64, 32, 32],\n",
    "    dense_layers=[256, 16, 32, 32, 64],\n",
    "    dense_dropouts=[0.0, 0.0, 0.0, 0.0, 0.0],\n",
    ")\n",
    "ff_mlp_settings = MLPSettings(\n",
    "    embedding_outputs=[16 for _ in range(len(global_categorical_maps))],\n",
    "    numerical_dense_layers=[64, 16, 32],\n",
    "    dense_layers=[64, 128, 64, 256, 32],\n",
    "    dense_dropouts=[0.0, 0.0, 0.0, 0.0, 0.0],\n",
    ")\n",
    "lut_mlp_settings = MLPSettings(\n",
    "    embedding_outputs=[16 for _ in range(len(global_categorical_maps))],\n",
    "    numerical_dense_layers=[64, 16, 32, 32],\n",
    "    dense_layers=[64, 128, 128, 64],\n",
    "    dense_dropouts=[0.0, 0.0, 0.0, 0.0],\n",
    ")\n",
    "cycles_mlp_settings = MLPSettings(\n",
    "    embedding_outputs=[16 for _ in range(len(global_categorical_maps))],\n",
    "    numerical_dense_layers=[32, 16, 64],\n",
    "    dense_layers=[256, 32, 32, 32, 256, 128, 128, 32, 16, 16, 64],\n",
    "    dense_dropouts=[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    ")\n",
    "\n",
    "vsynth_bram_mlp_settings = MLPSettings(\n",
    "    embedding_outputs=[16, 4],\n",
    "    numerical_dense_layers=[32, 16, 16, 16],\n",
    "    dense_layers=[16, 16, 16, 16, 16, 16, 16, 16],\n",
    "    dense_dropouts=[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    ")\n",
    "vsynth_dsp_mlp_settings = MLPSettings(\n",
    "    embedding_outputs=[16 for _ in range(len(global_categorical_maps))],\n",
    "    numerical_dense_layers=[16, 256, 64, 256, 64],\n",
    "    dense_layers=[16, 256, 128, 16, 16, 64],\n",
    "    dense_dropouts=[0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    ")\n",
    "vsynth_ff_mlp_settings = MLPSettings(\n",
    "    embedding_outputs=[16 for _ in range(len(global_categorical_maps))],\n",
    "    numerical_dense_layers=[64, 256, 128, 16, 256],\n",
    "    dense_layers=[64, 128, 128, 16, 32, 256, 64],\n",
    "    dense_dropouts=[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    ")\n",
    "vsynth_lut_mlp_settings = MLPSettings(\n",
    "    embedding_outputs=[8, 16],\n",
    "    numerical_dense_layers=[256],\n",
    "    dense_layers=[64, 32, 16, 32, 32],\n",
    "    dense_dropouts=[0.0, 0.0, 0.0, 0.0],\n",
    ")\n",
    "vsynth_cycles_mlp_settings = MLPSettings(\n",
    "    embedding_outputs=[16 for _ in range(len(global_categorical_maps))],\n",
    "    numerical_dense_layers=[64, 256, 128, 16, 256],\n",
    "    dense_layers=[64, 128, 128, 16, 32, 256, 64],\n",
    "    dense_dropouts=[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    ")\n",
    "\n",
    "mlp_settings = vsynth_lut_mlp_settings\n",
    "\n",
    "model_wrapper = ModelWrapper()\n",
    "model_wrapper.build_mlp_model(\n",
    "    mlp_settings,\n",
    "    input_shape=input_shape,\n",
    "    output_shape=output_shape,\n",
    "    categorical_maps=global_categorical_maps,\n",
    "    model_name=f\"{'-'.join([x.upper() for x in target_labels])}_MLP\",\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rule4ml.models.estimators import TrainSettings\n",
    "from rule4ml.models.metrics import parametric_smape, parametric_r2\n",
    "\n",
    "smape = parametric_smape(0, \"-\".join([x.upper() for x in target_labels]))\n",
    "r2 = parametric_r2(0, \"-\".join([x.upper() for x in target_labels]))\n",
    "\n",
    "bram_train_settings = TrainSettings(\n",
    "    num_epochs=50,\n",
    "    batch_size=64,\n",
    "    learning_rate=1e-4,\n",
    "    loss_function=\"mae\",\n",
    "    metrics=[smape, r2],\n",
    ")\n",
    "dsp_train_settings = TrainSettings(\n",
    "    num_epochs=50,\n",
    "    batch_size=32,\n",
    "    learning_rate=1e-4,\n",
    "    loss_function=\"mae\",\n",
    "    metrics=[smape, r2],\n",
    ")\n",
    "ff_train_settings = TrainSettings(\n",
    "    num_epochs=50,\n",
    "    batch_size=64,\n",
    "    learning_rate=1e-4,\n",
    "    loss_function=\"mae\",\n",
    "    metrics=[smape, r2],\n",
    ")\n",
    "lut_train_settings = TrainSettings(\n",
    "    num_epochs=50,\n",
    "    batch_size=32,\n",
    "    learning_rate=1e-4,\n",
    "    loss_function=\"mae\",\n",
    "    metrics=[smape, r2],\n",
    ")\n",
    "cycles_train_settings = TrainSettings(\n",
    "    num_epochs=50,\n",
    "    batch_size=64,\n",
    "    learning_rate=1e-3,\n",
    "    loss_function=\"mae\",\n",
    "    metrics=[smape, r2],\n",
    ")\n",
    "\n",
    "vsynth_bram_train_settings = TrainSettings(\n",
    "    num_epochs=20,\n",
    "    batch_size=128,\n",
    "    learning_rate=1e-3,\n",
    "    loss_function=\"msle\",\n",
    "    metrics=[smape, r2],\n",
    ")\n",
    "vsynth_dsp_train_settings = TrainSettings(\n",
    "    num_epochs=20,\n",
    "    batch_size=256,\n",
    "    learning_rate=1e-4,\n",
    "    loss_function=\"msle\",\n",
    "    metrics=[smape, r2],\n",
    ")\n",
    "vsynth_ff_train_settings = TrainSettings(\n",
    "    num_epochs=20,\n",
    "    batch_size=128,\n",
    "    learning_rate=1e-4,\n",
    "    loss_function=\"msle\",\n",
    "    metrics=[smape, r2],\n",
    ")\n",
    "vsynth_lut_train_settings = TrainSettings(\n",
    "    num_epochs=20,\n",
    "    batch_size=64,\n",
    "    learning_rate=1e-3,\n",
    "    loss_function=\"msle\",\n",
    "    metrics=[smape, r2],\n",
    ")\n",
    "vsynth_cycles_train_settings = TrainSettings(\n",
    "    num_epochs=20,\n",
    "    batch_size=128,\n",
    "    learning_rate=1e-4,\n",
    "    loss_function=\"msle\",\n",
    "    metrics=[smape, r2],\n",
    ")\n",
    "\n",
    "train_settings = vsynth_lut_train_settings\n",
    "\n",
    "model_wrapper.build_dataset(\n",
    "    train_inputs_df,\n",
    "    train_targets_df,\n",
    "    train_settings.batch_size,\n",
    "    val_ratio=0.2,\n",
    "    train_repeats=10,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "11290/11290 [==============================] - 13s 1ms/step - loss: 79.0890 - smape_LUT: 100.0000 - r2_LUT: -8.8747 - val_loss: 79.1379 - val_smape_LUT: 100.0000 - val_r2_LUT: -9.0398\n",
      "Epoch 2/20\n",
      "11290/11290 [==============================] - 12s 1ms/step - loss: 79.0888 - smape_LUT: 100.0000 - r2_LUT: -8.8748 - val_loss: 79.1379 - val_smape_LUT: 100.0000 - val_r2_LUT: -9.0398\n",
      "Epoch 3/20\n",
      "11290/11290 [==============================] - 12s 1ms/step - loss: 79.0887 - smape_LUT: 100.0000 - r2_LUT: -8.8783 - val_loss: 79.1379 - val_smape_LUT: 100.0000 - val_r2_LUT: -9.0398\n",
      "Epoch 4/20\n",
      "11290/11290 [==============================] - 12s 1ms/step - loss: 79.0889 - smape_LUT: 100.0000 - r2_LUT: -8.8688 - val_loss: 79.1379 - val_smape_LUT: 100.0000 - val_r2_LUT: -9.0398\n",
      "Epoch 5/20\n",
      "11290/11290 [==============================] - 12s 1ms/step - loss: 79.0888 - smape_LUT: 100.0000 - r2_LUT: -8.8737 - val_loss: 79.1379 - val_smape_LUT: 100.0000 - val_r2_LUT: -9.0398\n",
      "Epoch 6/20\n",
      "11290/11290 [==============================] - 12s 1ms/step - loss: 79.0890 - smape_LUT: 100.0000 - r2_LUT: -8.8782 - val_loss: 79.1379 - val_smape_LUT: 100.0000 - val_r2_LUT: -9.0398\n",
      "Epoch 7/20\n",
      "11290/11290 [==============================] - 12s 1ms/step - loss: 79.0888 - smape_LUT: 100.0000 - r2_LUT: -8.8728 - val_loss: 79.1379 - val_smape_LUT: 100.0000 - val_r2_LUT: -9.0398\n",
      "Epoch 8/20\n",
      "11290/11290 [==============================] - 12s 1ms/step - loss: 79.0888 - smape_LUT: 100.0000 - r2_LUT: -8.8680 - val_loss: 79.1379 - val_smape_LUT: 100.0000 - val_r2_LUT: -9.0398\n",
      "Epoch 9/20\n",
      "11290/11290 [==============================] - 12s 1ms/step - loss: 79.0889 - smape_LUT: 100.0000 - r2_LUT: -8.8795 - val_loss: 79.1379 - val_smape_LUT: 100.0000 - val_r2_LUT: -9.0398\n",
      "Epoch 10/20\n",
      "11290/11290 [==============================] - 12s 1ms/step - loss: 79.0888 - smape_LUT: 100.0000 - r2_LUT: -8.8701 - val_loss: 79.1379 - val_smape_LUT: 100.0000 - val_r2_LUT: -9.0398\n",
      "Epoch 11/20\n",
      "11290/11290 [==============================] - 12s 1ms/step - loss: 79.0888 - smape_LUT: 100.0000 - r2_LUT: -8.8725 - val_loss: 79.1379 - val_smape_LUT: 100.0000 - val_r2_LUT: -9.0398\n",
      "Epoch 12/20\n",
      "11290/11290 [==============================] - 12s 1ms/step - loss: 79.0891 - smape_LUT: 100.0000 - r2_LUT: -8.8723 - val_loss: 79.1379 - val_smape_LUT: 100.0000 - val_r2_LUT: -9.0398\n",
      "Epoch 13/20\n",
      "11290/11290 [==============================] - 12s 1ms/step - loss: 79.0891 - smape_LUT: 100.0000 - r2_LUT: -8.8723 - val_loss: 79.1379 - val_smape_LUT: 100.0000 - val_r2_LUT: -9.0398\n",
      "Epoch 14/20\n",
      "11290/11290 [==============================] - 12s 1ms/step - loss: 79.0888 - smape_LUT: 100.0000 - r2_LUT: -8.8737 - val_loss: 79.1379 - val_smape_LUT: 100.0000 - val_r2_LUT: -9.0398\n",
      "Epoch 15/20\n",
      "11290/11290 [==============================] - 12s 1ms/step - loss: 79.0887 - smape_LUT: 100.0000 - r2_LUT: -8.8711 - val_loss: 79.1379 - val_smape_LUT: 100.0000 - val_r2_LUT: -9.0398\n",
      "Epoch 16/20\n",
      "11290/11290 [==============================] - 12s 1ms/step - loss: 79.0888 - smape_LUT: 100.0000 - r2_LUT: -8.8787 - val_loss: 79.1379 - val_smape_LUT: 100.0000 - val_r2_LUT: -9.0398\n",
      "Epoch 17/20\n",
      "11290/11290 [==============================] - 12s 1ms/step - loss: 79.0888 - smape_LUT: 100.0000 - r2_LUT: -8.8760 - val_loss: 79.1379 - val_smape_LUT: 100.0000 - val_r2_LUT: -9.0398\n",
      "Epoch 18/20\n",
      "11290/11290 [==============================] - 12s 1ms/step - loss: 79.0889 - smape_LUT: 100.0000 - r2_LUT: -8.8754 - val_loss: 79.1379 - val_smape_LUT: 100.0000 - val_r2_LUT: -9.0398\n",
      "Epoch 19/20\n",
      "11290/11290 [==============================] - 12s 1ms/step - loss: 79.0889 - smape_LUT: 100.0000 - r2_LUT: -8.8737 - val_loss: 79.1379 - val_smape_LUT: 100.0000 - val_r2_LUT: -9.0398\n",
      "Epoch 20/20\n",
      "11290/11290 [==============================] - 12s 1ms/step - loss: 79.0889 - smape_LUT: 100.0000 - r2_LUT: -8.8721 - val_loss: 79.1379 - val_smape_LUT: 100.0000 - val_r2_LUT: -9.0398\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import LearningRateScheduler, ModelCheckpoint, TensorBoard\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "start_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "log_dir = os.path.join(\"./logs\", f\"{model_wrapper.model.name}_{start_time}\")\n",
    "checkpoint_dir = os.path.join(\"./checkpoints\", f\"{model_wrapper.model.name}_{start_time}\")\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "checkpoint_file = os.path.join(checkpoint_dir, f\"{'-'.join(target_labels)}_best.weights.h5\")\n",
    "\n",
    "tensorboard_callback = TensorBoard(\n",
    "    log_dir=log_dir,\n",
    "    histogram_freq=1,\n",
    "    write_graph=True,\n",
    "    write_images=True,\n",
    "    write_steps_per_second=False,\n",
    "    update_freq=\"epoch\",\n",
    "    embeddings_freq=1,\n",
    ")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_file,\n",
    "    save_weights_only=True,\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    save_best_only=True,\n",
    ")\n",
    "\n",
    "\n",
    "def scheduler(epoch, lr):\n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        return lr * np.exp(-0.2)\n",
    "    return lr\n",
    "\n",
    "\n",
    "lr_callback = LearningRateScheduler(scheduler)\n",
    "\n",
    "callbacks = [\n",
    "    tensorboard_callback,\n",
    "    checkpoint_callback,\n",
    "    # lr_callback\n",
    "]\n",
    "\n",
    "fit_history = model_wrapper.fit(train_settings, callbacks=callbacks, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_wrapper.save(\"./models/vsynth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Finetuning (Optional) <a class=\"anchor\" id=\"finetune\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 Finetuning an MLP <a class=\"anchor\" id=\"finetune-mlp\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rule4ml.models.tuning import Searcher\n",
    "from rule4ml.models.estimators import ModelWrapper\n",
    "\n",
    "target_labels = [\"lut\"]\n",
    "\n",
    "train_targets_df = train_df[target_labels].copy()\n",
    "test_targets_df = test_df[target_labels].copy()\n",
    "\n",
    "model_wrapper = ModelWrapper()\n",
    "searcher = Searcher(model_wrapper)\n",
    "searcher.mlp_search(\n",
    "    train_inputs_df,\n",
    "    train_targets_df,\n",
    "    global_categorical_maps,\n",
    "    directory=\"./mlp_search\",\n",
    "    verbose=1,\n",
    ")\n",
    "searcher.tuner.results_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 Loading and Retraining <a class=\"anchor\" id=\"load-tuner\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rule4ml.models.tuning import Searcher\n",
    "from rule4ml.models.estimators import ModelWrapper\n",
    "\n",
    "model_wrapper = ModelWrapper()\n",
    "searcher = Searcher(model_wrapper)\n",
    "searcher.load_tuner(\n",
    "    train_inputs_df,\n",
    "    train_targets_df,\n",
    "    global_categorical_maps,\n",
    "    \"./mlp_search\",\n",
    "    \"20240930-040140\",\n",
    ")\n",
    "searcher.tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rule4ml.models.estimators import TrainSettings\n",
    "\n",
    "model_wrapper = searcher.model_wrapper\n",
    "model_wrapper.fit(searcher.train_settings, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Testing Prediction Models <a class=\"anchor\" id=\"test-models\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Benchmark Networks <a class=\"anchor\" id=\"benchmark-test\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import (\n",
    "    Dense,\n",
    "    Add,\n",
    "    Input,\n",
    "    BatchNormalization,\n",
    "    Conv2D,\n",
    "    Flatten,\n",
    ")\n",
    "\n",
    "\n",
    "def get_test_model(name):\n",
    "    model = None\n",
    "    if name == \"jet\":\n",
    "        input_size = 16\n",
    "        inputs = Input(shape=(input_size,))\n",
    "        x = Dense(32, use_bias=True)(inputs)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Dense(32, use_bias=True)(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Dense(32, use_bias=True)(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Dense(5, use_bias=True)(x)\n",
    "        outputs = Activation(\"softmax\")(x)\n",
    "\n",
    "        model = keras.Model(inputs=inputs, outputs=outputs, name=name)\n",
    "        model.build([None, input_size])\n",
    "\n",
    "    if name == \"quarks\":\n",
    "        input_size = 10\n",
    "        inputs = Input(shape=(input_size,))\n",
    "        x = Dense(32, use_bias=True)(inputs)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Dense(1, use_bias=True)(x)\n",
    "        outputs = Activation(\"sigmoid\")(x)\n",
    "\n",
    "        model = keras.Model(inputs=inputs, outputs=outputs, name=name)\n",
    "        model.build([None, input_size])\n",
    "\n",
    "    if name == \"anomaly\":\n",
    "        input_size = 128\n",
    "        inputs = Input(shape=(input_size,))\n",
    "        x = Dense(8, use_bias=True)(inputs)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Dense(4, use_bias=True)(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Dense(128, use_bias=True)(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Dense(4, use_bias=True)(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Dense(128, use_bias=True)(x)\n",
    "        outputs = Activation(\"softmax\")(x)\n",
    "\n",
    "        model = keras.Model(inputs=inputs, outputs=outputs, name=name)\n",
    "        model.build([None, input_size])\n",
    "\n",
    "    if name == \"bipc\":\n",
    "        input_size = 36\n",
    "        inputs = Input(shape=(input_size,))\n",
    "        x = Dense(36, use_bias=False)(inputs)\n",
    "\n",
    "        y = Activation(\"relu\")(x)\n",
    "        for i in range(5):\n",
    "            y = Dense(36, use_bias=False)(y)\n",
    "            y = Add()([x, y])\n",
    "            y = Activation(\"relu\")(y)\n",
    "        outputs = y\n",
    "\n",
    "        model = keras.Model(inputs=inputs, outputs=outputs, name=name)\n",
    "        model.build([None, input_size])\n",
    "\n",
    "    if name == \"cookie\":\n",
    "        input_size = 512\n",
    "        inputs = Input(shape=(input_size,))\n",
    "        x = Dense(4, use_bias=True)(inputs)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Dense(32, use_bias=True)(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Dense(32, use_bias=True)(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Dense(5, use_bias=True)(x)\n",
    "        outputs = Activation(\"softmax\")(x)\n",
    "\n",
    "        model = keras.Model(inputs=inputs, outputs=outputs, name=name)\n",
    "        model.build([None, input_size])\n",
    "\n",
    "    if name == \"mnist\":\n",
    "        input_size = 784\n",
    "        inputs = Input(shape=(input_size,))\n",
    "        x = Dense(16, use_bias=True)(inputs)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Dense(10, use_bias=True)(x)\n",
    "        outputs = Activation(\"softmax\")(x)\n",
    "\n",
    "        model = keras.Model(inputs=inputs, outputs=outputs, name=name)\n",
    "        model.build([None, input_size])\n",
    "\n",
    "    if name == \"automlp\":\n",
    "        input_size = 7\n",
    "        inputs = Input(shape=(input_size,))\n",
    "        x = Dense(12, use_bias=True)(inputs)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Dense(16, use_bias=True)(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Dense(12, use_bias=True)(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Dense(2, use_bias=True)(x)\n",
    "        outputs = Activation(\"softmax\")(x)\n",
    "\n",
    "        model = keras.Model(inputs=inputs, outputs=outputs, name=name)\n",
    "        model.build([None, input_size])\n",
    "\n",
    "    if name == \"particle\":\n",
    "        input_size = 14\n",
    "        inputs = Input(shape=(input_size,))\n",
    "        x = Dense(32, use_bias=True)(inputs)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Dense(32, use_bias=True)(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Dense(32, use_bias=True)(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Dense(3, use_bias=True)(x)\n",
    "        outputs = Activation(\"softmax\")(x)\n",
    "\n",
    "        model = keras.Model(inputs=inputs, outputs=outputs, name=name)\n",
    "        model.build([None, input_size])\n",
    "\n",
    "    if name == \"custom1\":\n",
    "        input_size = 16\n",
    "        inputs = Input(shape=(input_size,))\n",
    "        x = Dense(64, use_bias=True)(inputs)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Dense(32, use_bias=True)(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Dense(32, use_bias=True)(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Dense(32, use_bias=True)(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Dense(10, use_bias=True)(x)\n",
    "        outputs = Activation(\"softmax\")(x)\n",
    "\n",
    "        model = keras.Model(inputs=inputs, outputs=outputs, name=name)\n",
    "        model.build([None, input_size])\n",
    "\n",
    "    if name == \"custom2\":\n",
    "        input_size = 128\n",
    "        inputs = Input(shape=(input_size,))\n",
    "        x = Dense(16, use_bias=True)(inputs)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Dense(64, use_bias=True)(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Dense(32, use_bias=True)(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Dense(64, use_bias=True)(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Dense(32, use_bias=True)(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Dense(50, use_bias=True)(x)\n",
    "        outputs = Activation(\"softmax\")(x)\n",
    "\n",
    "        model = keras.Model(inputs=inputs, outputs=outputs, name=name)\n",
    "        model.build([None, input_size])\n",
    "\n",
    "    if name == \"custom3\":\n",
    "\n",
    "        def residual_block(x, units):\n",
    "            y = Dense(units)(x)\n",
    "            y = BatchNormalization()(y)\n",
    "            y = Activation(\"relu\")(y)\n",
    "\n",
    "            y = Dense(units)(y)\n",
    "            y = BatchNormalization()(y)\n",
    "\n",
    "            if x.shape[-1] == units:\n",
    "                y = Add()([x, y])\n",
    "            else:\n",
    "                x = Dense(units)(x)\n",
    "                x = BatchNormalization()(x)\n",
    "                y = Add()([x, y])\n",
    "\n",
    "            y = Activation(\"relu\")(y)\n",
    "            return y\n",
    "\n",
    "        input_size = 64\n",
    "        inputs = Input(shape=(input_size,))\n",
    "        x = Dense(32, use_bias=True)(inputs)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "\n",
    "        x = residual_block(x, units=32)\n",
    "        x = residual_block(x, units=32)\n",
    "\n",
    "        x = Dense(10)(x)\n",
    "        outputs = Activation(\"softmax\")(x)\n",
    "\n",
    "        model = keras.Model(inputs=inputs, outputs=outputs, name=name)\n",
    "        model.build([None, input_size])\n",
    "\n",
    "    if name == \"conv2d-nopool\":\n",
    "        input_size = (16, 16, 1)\n",
    "        inputs = Input(input_size)\n",
    "        x = Conv2D(16, (3, 3), padding=\"same\")(inputs)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Conv2D(4, (3, 3), padding=\"same\")(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(2, use_bias=True)(x)\n",
    "        outputs = Activation(\"softmax\")(x)\n",
    "\n",
    "        model = keras.Model(inputs=inputs, outputs=outputs, name=name)\n",
    "        model.build([None, input_size])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rule4ml.models.estimators import ModelWrapper, MultiModelEstimator\n",
    "import itertools\n",
    "\n",
    "hls_configs = [\n",
    "    {\n",
    "        \"model\": {\n",
    "            \"precision\": \"ap_fixed<8, 3>\",\n",
    "            \"reuse_factor\": 32,\n",
    "            \"strategy\": strategy,\n",
    "            \"bram_factor\": 1000000000,\n",
    "            \"trace_output\": False,\n",
    "        },\n",
    "        \"clock_period\": 10.0,\n",
    "        \"io_type\": \"io_parallel\",\n",
    "        \"board\": board,\n",
    "    }\n",
    "    for board, strategy in itertools.product([\"pynq-z2\", \"zcu102\"], [\"Latency\", \"Resource\"])\n",
    "]\n",
    "\n",
    "model_names = [\n",
    "    \"jet\",\n",
    "    \"quarks\",\n",
    "    \"anomaly\",\n",
    "    \"bipc\",\n",
    "    \"cookie\",\n",
    "    \"mnist\",\n",
    "    \"automlp\",\n",
    "    \"particle\",\n",
    "    \"custom1\",\n",
    "    \"custom2\",\n",
    "    \"custom3\",\n",
    "]\n",
    "models = [get_test_model(name) for name in model_names]\n",
    "\n",
    "target_labels = [\"bram\", \"dsp\", \"ff\", \"lut\", \"cycles\"]\n",
    "\n",
    "estimator = MultiModelEstimator()\n",
    "for label in target_labels:\n",
    "    model_wrapper = ModelWrapper()\n",
    "    model_wrapper.load(\n",
    "        f\"./models/best_{label.upper()}_MLP_config.json\",\n",
    "        f\"./models/best_{label.upper()}_MLP.weights.h5\",\n",
    "    )\n",
    "    estimator.add_model_wrapper(model_wrapper)\n",
    "\n",
    "prediction_df = estimator.predict(models, hls_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df.sort_values([\"Board\", \"Strategy\", \"Reuse Factor\"]).round(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Plots <a class=\"anchor\" id=\"plots\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 Box Plots <a class=\"anchor\" id=\"box-plots\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "\n",
    "from rule4ml.parsers.data_parser import (\n",
    "    read_from_json,\n",
    "    get_global_data,\n",
    "    get_sequential_data,\n",
    "    to_dataframe,\n",
    ")\n",
    "\n",
    "from rule4ml.parsers.data_parser import (\n",
    "    default_board_map,\n",
    "    default_strategy_map,\n",
    "    default_layer_type_map,\n",
    ")\n",
    "\n",
    "base_path = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "json_data = read_from_json(\n",
    "    os.path.join(base_path, \"datasets\", \"benchmark_vsynth_data.json\"),\n",
    ")\n",
    "\n",
    "meta_data, global_inputs, targets = get_global_data(json_data)\n",
    "sequential_inputs = get_sequential_data(json_data)\n",
    "\n",
    "# Ordinal encoding of categorical inputs\n",
    "global_categorical_maps = {\n",
    "    \"strategy\": default_strategy_map,\n",
    "    \"board\": default_board_map,\n",
    "}\n",
    "sequential_categorical_maps = {\n",
    "    \"layer_type\": default_layer_type_map,\n",
    "}\n",
    "\n",
    "test_df = to_dataframe(\n",
    "    meta_data=meta_data,\n",
    "    global_inputs=global_inputs,\n",
    "    sequential_inputs=sequential_inputs,\n",
    "    global_categorical_maps=global_categorical_maps,\n",
    "    sequential_categorical_maps=sequential_categorical_maps,\n",
    "    targets=targets,\n",
    ")\n",
    "\n",
    "seed_num = 1337\n",
    "np.random.seed(seed_num)\n",
    "keras.utils.set_random_seed(seed_num)\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "feature_labels = [\n",
    "    \"strategy\",\n",
    "    \"board\",\n",
    "    # \"precision\",\n",
    "    # \"model_total_bits\",\n",
    "    # \"model_integer_bits\",\n",
    "    # \"model_fractional_bits\",\n",
    "    \"reuse_mean\",\n",
    "    \"weight_total_bits\",\n",
    "    # \"weight_integer_bits\",\n",
    "    # \"weight_fractional_bits\",\n",
    "    # \"dense_count\",\n",
    "    # \"batchnormalization_count\",\n",
    "    # \"add_count\",\n",
    "    # \"concatenate_count\",\n",
    "    # \"dropout_count\",\n",
    "    # \"relu_count\",\n",
    "    # \"sigmoid_count\",\n",
    "    # \"tanh_count\",\n",
    "    # \"softmax_count\",\n",
    "    # \"dense_parameters_mean\",\n",
    "    # \"dense_inputs_mean\",\n",
    "    # \"dense_outputs_mean\",\n",
    "    # \"dense_reuse_mean\",\n",
    "    \"dense_inputs_mean\",\n",
    "    # \"dense_inputs_min\",\n",
    "    # \"dense_inputs_min_idx\",\n",
    "    # \"dense_inputs_max\",\n",
    "    # \"dense_inputs_max_idx\",\n",
    "    \"dense_outputs_mean\",\n",
    "    # \"dense_outputs_min\",\n",
    "    # \"dense_outputs_min_idx\",\n",
    "    # \"dense_outputs_max\",\n",
    "    # \"dense_outputs_max_idx\",\n",
    "    \"dense_parameters_mean\",\n",
    "    # \"dense_parameters_min\",\n",
    "    # \"dense_parameters_min_idx\",\n",
    "    # \"dense_parameters_max\",\n",
    "    # \"dense_parameters_max_idx\",\n",
    "    \"dense_reuse_mean\",\n",
    "    # \"dense_reuse_min\",\n",
    "    # \"dense_reuse_min_idx\",\n",
    "    # \"dense_reuse_max\",\n",
    "    # \"dense_reuse_max_idx\",\n",
    "    \"dense_count\",\n",
    "    # \"batchnormalization_inputs_mean\",\n",
    "    # \"batchnormalization_inputs_min\",\n",
    "    # \"batchnormalization_inputs_min_idx\",\n",
    "    # \"batchnormalization_inputs_max\",\n",
    "    # \"batchnormalization_inputs_max_idx\",\n",
    "    # \"batchnormalization_outputs_mean\",\n",
    "    # \"batchnormalization_outputs_min\",\n",
    "    # \"batchnormalization_outputs_min_idx\",\n",
    "    # \"batchnormalization_outputs_max\",\n",
    "    # \"batchnormalization_outputs_max_idx\",\n",
    "    # \"batchnormalization_parameters_mean\",\n",
    "    # \"batchnormalization_parameters_min\",\n",
    "    # \"batchnormalization_parameters_min_idx\",\n",
    "    # \"batchnormalization_parameters_max\",\n",
    "    # \"batchnormalization_parameters_max_idx\",\n",
    "    # \"batchnormalization_count\",\n",
    "    # \"add_inputs_mean\",\n",
    "    # \"add_inputs_min\",\n",
    "    # \"add_inputs_min_idx\",\n",
    "    # \"add_inputs_max\",\n",
    "    # \"add_inputs_max_idx\",\n",
    "    # \"add_outputs_mean\",\n",
    "    # \"add_outputs_min\",\n",
    "    # \"add_outputs_min_idx\",\n",
    "    # \"add_outputs_max\",\n",
    "    # \"add_outputs_max_idx\",\n",
    "    # \"add_count\",\n",
    "    # \"concatenate_inputs_mean\",\n",
    "    # \"concatenate_inputs_min\",\n",
    "    # \"concatenate_inputs_min_idx\",\n",
    "    # \"concatenate_inputs_max\",\n",
    "    # \"concatenate_inputs_max_idx\",\n",
    "    # \"concatenate_outputs_mean\",\n",
    "    # \"concatenate_outputs_min\",\n",
    "    # \"concatenate_outputs_min_idx\",\n",
    "    # \"concatenate_outputs_max\",\n",
    "    # \"concatenate_outputs_max_idx\",\n",
    "    # \"concatenate_count\",\n",
    "    # \"dropout_inputs_mean\",\n",
    "    # \"dropout_inputs_min\",\n",
    "    # \"dropout_inputs_min_idx\",\n",
    "    # \"dropout_inputs_max\",\n",
    "    # \"dropout_inputs_max_idx\",\n",
    "    # \"dropout_outputs_mean\",\n",
    "    # \"dropout_outputs_min\",\n",
    "    # \"dropout_outputs_min_idx\",\n",
    "    # \"dropout_outputs_max\",\n",
    "    # \"dropout_outputs_max_idx\",\n",
    "    # \"dropout_count\",\n",
    "    # \"relu_inputs_mean\",\n",
    "    # \"relu_inputs_min\",\n",
    "    # \"relu_inputs_min_idx\",\n",
    "    # \"relu_inputs_max\",\n",
    "    # \"relu_inputs_max_idx\",\n",
    "    # \"relu_outputs_mean\",\n",
    "    # \"relu_outputs_min\",\n",
    "    # \"relu_outputs_min_idx\",\n",
    "    # \"relu_outputs_max\",\n",
    "    # \"relu_outputs_max_idx\",\n",
    "    # \"relu_count\",\n",
    "    # \"sigmoid_inputs_mean\",\n",
    "    # \"sigmoid_inputs_min\",\n",
    "    # \"sigmoid_inputs_min_idx\",\n",
    "    # \"sigmoid_inputs_max\",\n",
    "    # \"sigmoid_inputs_max_idx\",\n",
    "    # \"sigmoid_outputs_mean\",\n",
    "    # \"sigmoid_outputs_min\",\n",
    "    # \"sigmoid_outputs_min_idx\",\n",
    "    # \"sigmoid_outputs_max\",\n",
    "    # \"sigmoid_outputs_max_idx\",\n",
    "    # \"sigmoid_count\",\n",
    "    # \"tanh_inputs_mean\",\n",
    "    # \"tanh_inputs_min\",\n",
    "    # \"tanh_inputs_min_idx\",\n",
    "    # \"tanh_inputs_max\",\n",
    "    # \"tanh_inputs_max_idx\",\n",
    "    # \"tanh_outputs_mean\",\n",
    "    # \"tanh_outputs_min\",\n",
    "    # \"tanh_outputs_min_idx\",\n",
    "    # \"tanh_outputs_max\",\n",
    "    # \"tanh_outputs_max_idx\",\n",
    "    # \"tanh_count\",\n",
    "    # \"softmax_inputs_mean\",\n",
    "    # \"softmax_inputs_min\",\n",
    "    # \"softmax_inputs_min_idx\",\n",
    "    # \"softmax_inputs_max\",\n",
    "    # \"softmax_inputs_max_idx\",\n",
    "    # \"softmax_outputs_mean\",\n",
    "    # \"softmax_outputs_min\",\n",
    "    # \"softmax_outputs_min_idx\",\n",
    "    # \"softmax_outputs_max\",\n",
    "    # \"softmax_outputs_max_idx\",\n",
    "    # \"softmax_count\",\n",
    "    # \"total_mult\",\n",
    "    # \"total_add\",\n",
    "    # \"total_logical\",\n",
    "    # \"total_lookup\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rule4ml.models.estimators import ModelWrapper\n",
    "from rule4ml.parsers.data_parser import boards_data, default_board_map\n",
    "\n",
    "test_inputs_df = test_df[feature_labels].copy()\n",
    "print(f\"Test Inputs: {test_inputs_df.shape}\")\n",
    "\n",
    "prediction_labels = [\"bram\", \"dsp\", \"ff\", \"lut\", \"cycles\"]\n",
    "test_targets_df = test_df[prediction_labels].copy()\n",
    "\n",
    "wrappers = []\n",
    "abs_errors = []\n",
    "sape_errors = []\n",
    "normed_errors = []\n",
    "for label in prediction_labels:\n",
    "    wrapper = ModelWrapper()\n",
    "    # wrapper.load(\n",
    "    #     f\"./models/best_{label.upper()}_MLP_config.json\",\n",
    "    #     f\"./models/best_{label.upper()}_MLP.weights.h5\",\n",
    "    # )\n",
    "    wrapper.load(\n",
    "        f\"./models/vsynth/{label.upper()}_MLP_config.json\",\n",
    "        f\"./models/vsynth/{label.upper()}_MLP.weights.h5\",\n",
    "    )\n",
    "    wrappers.append(wrapper)\n",
    "\n",
    "    pred = wrapper.predict_from_df(test_inputs_df).squeeze()\n",
    "    gn = test_targets_df[label].values\n",
    "\n",
    "    abs_errors.append(np.abs(gn - pred))\n",
    "    sape_errors.append(2 * np.abs(gn - pred) / (np.abs(gn) + np.abs(pred) + 1e-6))\n",
    "\n",
    "    board_name = \"\"\n",
    "    for board, board_id in default_board_map.items():\n",
    "        if board_id == test_inputs_df[\"board\"].values[0]:\n",
    "            board_name = board\n",
    "            break\n",
    "\n",
    "    if label != \"cycles\":\n",
    "        max_vals = boards_data[board_name][f\"max_{label}\"]\n",
    "        normed_gn = np.maximum(1 / max_vals, np.minimum(gn / max_vals, 2.0)) * 100\n",
    "        normed_pred = np.maximum(1 / max_vals, np.minimum(pred / max_vals, 2.0)) * 100\n",
    "        normed_errors.append(np.abs(normed_gn - normed_pred))\n",
    "    else:\n",
    "        normed_errors.append(np.abs(gn - pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.asarray(abs_errors).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "plt.rcParams.update({\"font.size\": 16})\n",
    "\n",
    "fig, axis = plt.subplots(1, len(abs_errors), figsize=(12, 8))\n",
    "axis = np.reshape(axis, -1)\n",
    "fig.subplots_adjust(hspace=0.1, wspace=0.9)\n",
    "\n",
    "iqr_weight = 1.5\n",
    "colors = [\"pink\", \"yellow\", \"lightgreen\", \"lightblue\", \"#FFA500\"]\n",
    "\n",
    "for i, errors in enumerate(abs_errors):\n",
    "    label = prediction_labels[i]\n",
    "    ax = axis[i]\n",
    "    bplot = ax.boxplot(\n",
    "        errors,\n",
    "        whis=iqr_weight,\n",
    "        tick_labels=[label.upper()],\n",
    "        showfliers=True,\n",
    "        showmeans=True,\n",
    "        meanline=True,\n",
    "        vert=True,\n",
    "        patch_artist=True,\n",
    "    )\n",
    "\n",
    "    for j, patch in enumerate(bplot[\"boxes\"]):\n",
    "        patch.set_facecolor(colors[(i + j) % len(colors)])\n",
    "\n",
    "    # have more numbers of ticks than the original y-axis\n",
    "    ax.set_yticks(np.arange(0, np.max(errors) + 1, 10 ** np.round(np.log10(np.max(errors) // 10))))\n",
    "\n",
    "    ax.yaxis.grid(True)\n",
    "    ax.spines.top.set_visible(False)\n",
    "    ax.xaxis.tick_bottom()\n",
    "\n",
    "median_line = Line2D([0], [0], color=\"orange\", linestyle=\"--\", linewidth=1.5, label=\"Median\")\n",
    "mean_line = Line2D([0], [0], color=\"green\", linestyle=\"--\", linewidth=1.5, label=\"Mean\")\n",
    "\n",
    "handles = [median_line, mean_line]\n",
    "labels = [\"Median\", \"Mean\"]\n",
    "\n",
    "legends = fig.legend(\n",
    "    handles,\n",
    "    labels,\n",
    "    bbox_to_anchor=[0.9, 1],\n",
    "    loc=\"upper right\",\n",
    "    ncol=len(labels) // 2,\n",
    ")\n",
    "\n",
    "ytext = fig.text(0.05, 0.5, \"Absolute Error\", va=\"center\", rotation=\"vertical\", size=18)\n",
    "suptitle = fig.suptitle(\"Benchmark Errors - Boxplots\", fontsize=20, y=0.95)\n",
    "\n",
    "# fig.savefig(\n",
    "#     \"./box_plot_benchmark.png\",\n",
    "#     dpi=300,\n",
    "#     bbox_extra_artists=(legends, ytext, suptitle),\n",
    "#     bbox_inches=\"tight\",\n",
    "# )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "plt.rcParams.update({\"font.size\": 16})\n",
    "\n",
    "fig, axis = plt.subplots(2, 2, figsize=(12, 8), width_ratios=[3, 1])\n",
    "axis = np.reshape(axis, -1)\n",
    "fig.subplots_adjust(hspace=0.1, wspace=0.4)\n",
    "\n",
    "flier_ax, box_ax = axis[0], axis[2]\n",
    "\n",
    "iqr_weight = 1.5\n",
    "\n",
    "resources_errors = abs_errors[:4]\n",
    "resources_labels = prediction_labels[:4]\n",
    "\n",
    "threshold = 10.0\n",
    "below_threshold = []\n",
    "for errors in np.asarray(resources_errors):\n",
    "    below_threshold.append(np.sum(errors < threshold) / len(errors))\n",
    "print(f\"Resources below {threshold}%: {below_threshold}\")\n",
    "print(f\"Resources Mean: {np.mean(below_threshold)}\")\n",
    "\n",
    "bplot = box_ax.boxplot(\n",
    "    resources_errors,\n",
    "    whis=iqr_weight,\n",
    "    tick_labels=[x.upper() for x in resources_labels],\n",
    "    showfliers=True,\n",
    "    showmeans=True,\n",
    "    meanline=True,\n",
    "    vert=True,\n",
    "    patch_artist=True,\n",
    ")\n",
    "fliers = flier_ax.boxplot(\n",
    "    resources_errors,\n",
    "    whis=iqr_weight,\n",
    "    tick_labels=[\"\" for x in resources_labels],\n",
    "    showfliers=True,\n",
    "    showmeans=True,\n",
    "    meanline=True,\n",
    "    vert=True,\n",
    "    patch_artist=True,\n",
    ")\n",
    "\n",
    "colors = [\"pink\", \"yellow\", \"lightgreen\", \"lightblue\", \"FFA500\"]\n",
    "for patch, color in zip(bplot[\"boxes\"], colors):\n",
    "    patch.set_facecolor(color)\n",
    "\n",
    "# box_ax.set_ylim(-1, 30)\n",
    "# flier_ax.set_ylim(30, 200)\n",
    "\n",
    "# box_ax.yaxis.grid(True)\n",
    "# box_ax.spines.top.set_visible(False)\n",
    "# box_ax.xaxis.tick_bottom()\n",
    "# box_ax.set_yticks([0, 5, 10, 15, 20, 25, 30])\n",
    "\n",
    "# flier_ax.yaxis.grid(True)\n",
    "# flier_ax.spines.bottom.set_visible(False)\n",
    "# flier_ax.xaxis.tick_top()\n",
    "# flier_ax.set_yticks([30, 50, 75, 100, 125, 150, 175, 200])\n",
    "\n",
    "d = 0.5\n",
    "kwargs = dict(\n",
    "    marker=[(-1, -d), (1, d)],\n",
    "    markersize=12,\n",
    "    linestyle=\"none\",\n",
    "    color=\"k\",\n",
    "    mec=\"k\",\n",
    "    mew=1,\n",
    "    clip_on=False,\n",
    ")\n",
    "flier_ax.plot([0, 1], [0, 0], transform=flier_ax.transAxes, **kwargs)\n",
    "box_ax.plot([0, 1], [1, 1], transform=box_ax.transAxes, **kwargs)\n",
    "\n",
    "median_line = Line2D([0], [0], color=\"orange\", linestyle=\"--\", linewidth=1.5, label=\"Median\")\n",
    "mean_line = Line2D([0], [0], color=\"green\", linestyle=\"--\", linewidth=1.5, label=\"Mean\")\n",
    "\n",
    "handles = [median_line, mean_line]\n",
    "labels = [\"Median\", \"Mean\"]\n",
    "\n",
    "legends = fig.legend(\n",
    "    handles,\n",
    "    labels,\n",
    "    bbox_to_anchor=[0.9, 1],\n",
    "    # loc=\"upper left\",\n",
    "    loc=\"upper right\",\n",
    "    ncol=len(labels) // 2,\n",
    ")\n",
    "\n",
    "ytext = fig.text(0.06, 0.5, \"Error (%)\", va=\"center\", rotation=\"vertical\", size=18)\n",
    "suptitle = fig.suptitle(\"Prediction Errors - Boxplots\", fontsize=20, y=0.95)\n",
    "\n",
    "latency_flier_ax, latency_box_ax = axis[1], axis[3]\n",
    "\n",
    "iqr_weight = 1.5\n",
    "\n",
    "latency_errors = [abs_errors[4]]\n",
    "latency_labels = [prediction_labels[4]]\n",
    "\n",
    "threshold = 100.0\n",
    "below_threshold = []\n",
    "for errors in np.asarray(latency_errors):\n",
    "    below_threshold.append(np.sum(errors < threshold) / len(errors))\n",
    "print(f\"Latency below {threshold} cycles: {below_threshold}\")\n",
    "\n",
    "latency_bplot = latency_box_ax.boxplot(\n",
    "    latency_errors,\n",
    "    whis=iqr_weight,\n",
    "    widths=0.33,\n",
    "    tick_labels=[\"Cycles\"],\n",
    "    showfliers=True,\n",
    "    showmeans=True,\n",
    "    meanline=True,\n",
    "    vert=True,\n",
    "    patch_artist=True,\n",
    ")\n",
    "latency_fliers = latency_flier_ax.boxplot(\n",
    "    latency_errors,\n",
    "    whis=iqr_weight,\n",
    "    widths=0.33,\n",
    "    tick_labels=[\"\" for x in latency_labels],\n",
    "    showfliers=True,\n",
    "    showmeans=True,\n",
    "    meanline=True,\n",
    "    vert=True,\n",
    "    patch_artist=True,\n",
    ")\n",
    "\n",
    "colors = [\"lightblue\"]\n",
    "for patch, color in zip(latency_bplot[\"boxes\"], colors):\n",
    "    patch.set_facecolor(color)\n",
    "\n",
    "latency_box_ax.set_ylim(-10, 200)\n",
    "latency_flier_ax.set_ylim(200, 650)\n",
    "\n",
    "latency_box_ax.yaxis.grid(True)\n",
    "latency_box_ax.spines.top.set_visible(False)\n",
    "latency_box_ax.xaxis.tick_bottom()\n",
    "latency_box_ax.set_yticks(np.arange(0, 225, 25))\n",
    "\n",
    "latency_flier_ax.yaxis.grid(True)\n",
    "latency_flier_ax.spines.bottom.set_visible(False)\n",
    "latency_flier_ax.xaxis.tick_top()\n",
    "latency_flier_ax.set_yticks(np.arange(200, 700, 100))\n",
    "\n",
    "d = 0.5\n",
    "kwargs = dict(\n",
    "    marker=[(-1, -d), (1, d)],\n",
    "    markersize=12,\n",
    "    linestyle=\"none\",\n",
    "    color=\"k\",\n",
    "    mec=\"k\",\n",
    "    mew=1,\n",
    "    clip_on=False,\n",
    ")\n",
    "latency_flier_ax.plot([0, 1], [0, 0], transform=latency_flier_ax.transAxes, **kwargs)\n",
    "latency_box_ax.plot([0, 1], [1, 1], transform=latency_box_ax.transAxes, **kwargs)\n",
    "\n",
    "latency_ytext = fig.text(0.66, 0.5, \"Error (Cycles)\", va=\"center\", rotation=\"vertical\", size=18)\n",
    "\n",
    "resource_caption = fig.text(0.355, 0.04, \"(a)\", va=\"center\", size=18)\n",
    "latency_caption = fig.text(0.808, 0.04, \"(b)\", va=\"center\", size=18)\n",
    "\n",
    "# fig.savefig(\n",
    "#     \"/mnt/c/Users/Y540/Desktop/box_plot_merged.jpg\",\n",
    "#     dpi=300,\n",
    "#     bbox_extra_artists=(legends, ytext, suptitle, latency_ytext, resource_caption, latency_caption),\n",
    "#     bbox_inches=\"tight\",\n",
    "# )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 Bar Plots <a class=\"anchor\" id=\"bar-plots\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rule4ml.models.estimators import ModelWrapper, MultiModelEstimator\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "from rule4ml.parsers.data_parser import read_from_json\n",
    "\n",
    "base_path = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "json_data = read_from_json(os.path.join(base_path, \"datasets\", \"benchmark_vsynth_data.json\"))\n",
    "\n",
    "meta_data, global_inputs, targets = get_global_data(json_data)\n",
    "sequential_inputs = get_sequential_data(json_data)\n",
    "\n",
    "# Ordinal encoding of categorical inputs\n",
    "global_categorical_maps = {\n",
    "    \"strategy\": default_strategy_map,\n",
    "    \"board\": default_board_map,\n",
    "}\n",
    "sequential_categorical_maps = {\n",
    "    \"layer_type\": default_layer_type_map,\n",
    "}\n",
    "\n",
    "benchmark_df = to_dataframe(\n",
    "    meta_data=meta_data,\n",
    "    global_inputs=global_inputs,\n",
    "    sequential_inputs=sequential_inputs,\n",
    "    global_categorical_maps=global_categorical_maps,\n",
    "    sequential_categorical_maps=sequential_categorical_maps,\n",
    "    targets=targets,\n",
    ")\n",
    "benchmark_inputs_df = benchmark_df[feature_labels].copy()\n",
    "\n",
    "prediction_labels = [\"bram\", \"dsp\", \"ff\", \"lut\", \"cycles\"]\n",
    "benchmark_targets_df = benchmark_df[prediction_labels].copy()\n",
    "\n",
    "benchmark_merged_df = benchmark_df.copy().drop(columns=prediction_labels, axis=1)\n",
    "for label in prediction_labels:\n",
    "    wrapper = ModelWrapper()\n",
    "    wrapper.load(\n",
    "        f\"./models/vsynth/{label.upper()}_MLP_config.json\",\n",
    "        f\"./models/vsynth/{label.upper()}_MLP.weights.h5\",\n",
    "    )\n",
    "\n",
    "    benchmark_merged_df[f\"{label.upper()}_G\"] = benchmark_targets_df[label].values\n",
    "    benchmark_merged_df[f\"{label.upper()}_P\"] = wrapper.predict_from_df(\n",
    "        benchmark_inputs_df\n",
    "    ).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_order = [\"ap_fixed<2, 1>\", \"ap_fixed<8, 3>\", \"ap_fixed<16, 6>\"]\n",
    "benchmark_merged_df[\"model_precision\"] = pd.Categorical(\n",
    "    benchmark_merged_df[\"model_precision\"], categories=precision_order, ordered=True\n",
    ")\n",
    "\n",
    "benchmark_merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_strategy_map = {v: k for k, v in default_strategy_map.items()}\n",
    "reverse_board_map = {v: k for k, v in default_board_map.items()}\n",
    "\n",
    "benchmark_merged_df[\"strategy\"] = benchmark_merged_df[\"strategy\"].map(reverse_strategy_map)\n",
    "benchmark_merged_df[\"board\"] = benchmark_merged_df[\"board\"].map(reverse_board_map)\n",
    "\n",
    "prediction_grouped_mean = benchmark_merged_df.groupby(\n",
    "    [\"strategy\", \"board\", \"weight_total_bits\", \"global_reuse\"]\n",
    ")[\n",
    "    [\n",
    "        \"BRAM_G\",\n",
    "        \"BRAM_P\",\n",
    "        # \"DSP_G\",\n",
    "        # \"DSP_P\",\n",
    "        # \"FF_G\",\n",
    "        # \"FF_P\",\n",
    "        # \"LUT_G\",\n",
    "        # \"LUT_P\",\n",
    "        # \"CYCLES_G\",\n",
    "        # \"CYCLES_P\"\n",
    "    ]\n",
    "].mean()\n",
    "prediction_grouped_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams.update({\"font.size\": 14})\n",
    "\n",
    "board_name = \"alveo-u200\"\n",
    "grouped = prediction_grouped_mean.xs((board_name,), level=[\"board\"]).groupby(\n",
    "    [\"weight_total_bits\", \"strategy\"]\n",
    ")\n",
    "\n",
    "unique_precisions = benchmark_merged_df[\"weight_total_bits\"].unique()\n",
    "unique_strategies = benchmark_merged_df[\"strategy\"].unique()\n",
    "\n",
    "n_cols = len(unique_strategies)\n",
    "n_rows = len(unique_precisions)\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    n_rows, n_cols, dpi=300, figsize=(16, 10), squeeze=False, sharex=True, sharey=False\n",
    ")\n",
    "axes = axes.flatten()\n",
    "\n",
    "width = 0.11\n",
    "colors = [\"#008000\", \"#FF5964\", \"#17BEBB\", \"#FFA500\"]\n",
    "reuse_factors = sorted(benchmark_merged_df[\"global_reuse\"].unique())\n",
    "num_resources = 2\n",
    "resource_gap = 0\n",
    "\n",
    "total_width = num_resources * (2 * width + resource_gap) - resource_gap\n",
    "start = np.arange(1, len(reuse_factors) + 1) - total_width / 2\n",
    "\n",
    "row_idx = 0\n",
    "col_idx = 0\n",
    "for ax, ((precision, strategy), df) in zip(axes, grouped):\n",
    "    for i, (col_gn, col_pred) in enumerate(zip(df.columns[::2], df.columns[1::2])):\n",
    "        gn_vals = df[col_gn]\n",
    "        pred_vals = df[col_pred]\n",
    "\n",
    "        resource_indices = start + i * (2 * width + resource_gap)\n",
    "\n",
    "        for j, reuse_factor in enumerate(reuse_factors):\n",
    "            gn_label = \"\"\n",
    "            pred_label = \"\"\n",
    "            if j == 0:\n",
    "                gn_label = f\"{col_gn}\"\n",
    "                pred_label = f\"{col_pred}\"\n",
    "\n",
    "            ax.bar(\n",
    "                resource_indices[j] - width / 2,\n",
    "                gn_vals[j],\n",
    "                width,\n",
    "                label=gn_label,\n",
    "                color=colors[i % len(colors)],\n",
    "                edgecolor=\"black\",\n",
    "            )\n",
    "            ax.bar(\n",
    "                resource_indices[j] + width / 2,\n",
    "                pred_vals[j],\n",
    "                width,\n",
    "                label=pred_label,\n",
    "                color=colors[i % len(colors)],\n",
    "                edgecolor=\"black\",\n",
    "                hatch=\"///\",\n",
    "            )\n",
    "\n",
    "    ax.set_title(f\"{strategy}, {int(precision)}-bit width\")\n",
    "    ax.set_xticks(start + (num_resources - 1) * (width + resource_gap / 2))\n",
    "    ax.set_xticklabels(reuse_factors, rotation=45)\n",
    "\n",
    "    # if col_idx == 0:\n",
    "    #     ax.set_ylabel(\"Utilization (%)\")\n",
    "\n",
    "    # if row_idx == n_rows - 1:\n",
    "    #     ax.set_xlabel(\"Reuse Factor\")\n",
    "\n",
    "    col_idx += 1\n",
    "    if col_idx == n_cols:\n",
    "        row_idx += 1\n",
    "        col_idx = 0\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "legends = fig.legend(\n",
    "    handles,\n",
    "    labels,\n",
    "    title=\"Resources\",\n",
    "    bbox_to_anchor=[0.3, 1.03],\n",
    "    loc=\"upper left\",\n",
    "    # loc=\"upper right\",\n",
    "    ncol=len(labels) // 2,\n",
    ")\n",
    "\n",
    "xtext = fig.text(0.5, 0.035, \"Reuse Factor\", ha=\"center\", size=18)\n",
    "ytext = fig.text(0.07, 0.5, \"Utilization (%)\", va=\"center\", rotation=\"vertical\", size=18)\n",
    "\n",
    "suptitle = fig.suptitle(f\"{board_name}: Resource Utilization Trends\", fontsize=20, y=1.075)\n",
    "\n",
    "plt.subplots_adjust(hspace=0.275, wspace=0.125)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rule4ml.parsers.data_parser import (\n",
    "    read_from_json,\n",
    "    get_global_data,\n",
    "    get_sequential_data,\n",
    "    to_dataframe,\n",
    "    default_strategy_map,\n",
    "    default_board_map,\n",
    "    default_layer_type_map,\n",
    ")\n",
    "\n",
    "benchmark_data = read_from_json(\"../datasets/benchmark_data.json\")\n",
    "\n",
    "benchmark_meta_data, benchmark_global_inputs, benchmark_targets = get_global_data(benchmark_data)\n",
    "benchmark_sequential_inputs = get_sequential_data(benchmark_data)\n",
    "\n",
    "global_categorical_maps = {\n",
    "    \"strategy\": default_strategy_map,\n",
    "    \"board\": default_board_map,\n",
    "}\n",
    "sequential_categorical_maps = {\n",
    "    \"layer_type\": default_layer_type_map,\n",
    "}\n",
    "\n",
    "benchmark_df = to_dataframe(\n",
    "    meta_data=benchmark_meta_data,\n",
    "    global_inputs=benchmark_global_inputs,\n",
    "    sequential_inputs=benchmark_sequential_inputs,\n",
    "    global_categorical_maps={},\n",
    "    sequential_categorical_maps={},\n",
    "    targets=benchmark_targets,\n",
    ")\n",
    "benchmark_gn_df = benchmark_df[\n",
    "    [\n",
    "        \"model_name\",\n",
    "        \"board\",\n",
    "        \"strategy\",\n",
    "        \"precision\",\n",
    "        \"global_reuse\",\n",
    "        \"bram\",\n",
    "        \"dsp\",\n",
    "        \"ff\",\n",
    "        \"lut\",\n",
    "        \"cycles\",\n",
    "    ]\n",
    "].copy()\n",
    "benchmark_gn_df = benchmark_gn_df.rename(\n",
    "    {\n",
    "        \"model_name\": \"Model\",\n",
    "        \"board\": \"Board\",\n",
    "        \"strategy\": \"Strategy\",\n",
    "        \"precision\": \"Precision\",\n",
    "        \"global_reuse\": \"Reuse Factor\",\n",
    "        \"bram\": \"BRAM\",\n",
    "        \"dsp\": \"DSP\",\n",
    "        \"ff\": \"FF\",\n",
    "        \"lut\": \"LUT\",\n",
    "        \"cycles\": \"CYCLES\",\n",
    "    },\n",
    "    axis=1,\n",
    ")\n",
    "benchmark_gn_df.loc[benchmark_gn_df[\"Strategy\"] == \"latency\", \"Strategy\"] = \"Latency\"\n",
    "benchmark_gn_df.loc[benchmark_gn_df[\"Strategy\"] == \"resource\", \"Strategy\"] = \"Resource\"\n",
    "\n",
    "benchmark_gn_df[\"BRAM\"] = benchmark_gn_df[\"BRAM\"].apply(lambda x: min(x, 200.0))\n",
    "benchmark_gn_df[\"DSP\"] = benchmark_gn_df[\"DSP\"].apply(lambda x: min(x, 200.0))\n",
    "benchmark_gn_df[\"FF\"] = benchmark_gn_df[\"FF\"].apply(lambda x: min(x, 200.0))\n",
    "benchmark_gn_df[\"LUT\"] = benchmark_gn_df[\"LUT\"].apply(lambda x: min(x, 200.0))\n",
    "\n",
    "precision_order = [\"ap_fixed<2, 1>\", \"ap_fixed<8, 3>\", \"ap_fixed<16, 6>\"]\n",
    "benchmark_gn_df[\"Precision\"] = pd.Categorical(\n",
    "    benchmark_gn_df[\"Precision\"], categories=precision_order, ordered=True\n",
    ")\n",
    "\n",
    "benchmark_gn_df.head(14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gn_grouped_mean = (\n",
    "    benchmark_gn_df.groupby([\"Strategy\", \"Board\", \"Precision\", \"Reuse Factor\"])[\n",
    "        [\n",
    "            \"BRAM\",\n",
    "            \"DSP\",\n",
    "            \"FF\",\n",
    "            \"LUT\",\n",
    "            # \"CYCLES\"\n",
    "        ]\n",
    "    ]\n",
    "    .mean()\n",
    "    .round(0)\n",
    "    .astype(int)\n",
    ")\n",
    "\n",
    "prediction_grouped_mean = (\n",
    "    prediction_df.groupby([\"Strategy\", \"Board\", \"Precision\", \"Reuse Factor\"])[\n",
    "        [\n",
    "            \"BRAM\",\n",
    "            \"DSP\",\n",
    "            \"FF\",\n",
    "            \"LUT\",\n",
    "            # \"CYCLES\"\n",
    "        ]\n",
    "    ]\n",
    "    .mean()\n",
    "    .round(0)\n",
    "    .astype(int)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(\n",
    "    gn_grouped_mean,\n",
    "    prediction_grouped_mean,\n",
    "    on=(\"Strategy\", \"Board\", \"Precision\", \"Reuse Factor\"),\n",
    "    suffixes=(\" (G)\", \" (P)\"),\n",
    ")\n",
    "\n",
    "merged_df = merged_df[\n",
    "    [\n",
    "        \"BRAM (G)\",\n",
    "        \"BRAM (P)\",\n",
    "        \"DSP (G)\",\n",
    "        \"DSP (P)\",\n",
    "        \"FF (G)\",\n",
    "        \"FF (P)\",\n",
    "        \"LUT (G)\",\n",
    "        \"LUT (P)\",\n",
    "        # \"CYCLES (G)\", \"CYCLES (P)\",\n",
    "    ]\n",
    "]\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from rule4ml.parsers.utils import fixed_precision_to_bit_width\n",
    "\n",
    "plt.rcParams.update({\"font.size\": 14})\n",
    "\n",
    "grouped = merged_df.xs((\"pynq-z2\",), level=[\"Board\"]).groupby([\"Precision\", \"Strategy\"])\n",
    "\n",
    "n_groups = len(grouped)\n",
    "n_cols = 2\n",
    "n_rows = 3\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    n_rows, n_cols, dpi=300, figsize=(16, 10), squeeze=False, sharex=True, sharey=False\n",
    ")\n",
    "axes = axes.flatten()\n",
    "\n",
    "width = 0.11\n",
    "colors = [\"#008000\", \"#FF5964\", \"#17BEBB\", \"#FFA500\"]\n",
    "reuse_factors = prediction_df[\"Reuse Factor\"].unique()\n",
    "num_resources = 4\n",
    "resource_gap = 0\n",
    "\n",
    "total_width = num_resources * (2 * width + resource_gap) - resource_gap\n",
    "start = np.arange(1, len(reuse_factors) + 1) - total_width / 2\n",
    "\n",
    "row_idx = 0\n",
    "col_idx = 0\n",
    "for ax, ((precision, strategy), df) in zip(axes, grouped):\n",
    "    for i, (col_gn, col_pred) in enumerate(zip(df.columns[::2], df.columns[1::2])):\n",
    "        gn_vals = df[col_gn]\n",
    "        pred_vals = df[col_pred]\n",
    "\n",
    "        resource_indices = start + i * (2 * width + resource_gap)\n",
    "\n",
    "        for j, reuse_factor in enumerate(reuse_factors):\n",
    "            gn_label = \"\"\n",
    "            pred_label = \"\"\n",
    "            if j == 0:\n",
    "                gn_label = f\"{col_gn}\"\n",
    "                pred_label = f\"{col_pred}\"\n",
    "\n",
    "            ax.bar(\n",
    "                resource_indices[j] - width / 2,\n",
    "                gn_vals[j],\n",
    "                width,\n",
    "                label=gn_label,\n",
    "                color=colors[i % len(colors)],\n",
    "                edgecolor=\"black\",\n",
    "            )\n",
    "            ax.bar(\n",
    "                resource_indices[j] + width / 2,\n",
    "                pred_vals[j],\n",
    "                width,\n",
    "                label=pred_label,\n",
    "                color=colors[i % len(colors)],\n",
    "                edgecolor=\"black\",\n",
    "                hatch=\"///\",\n",
    "            )\n",
    "\n",
    "    total_bits, fraction_bits = fixed_precision_to_bit_width(precision)\n",
    "\n",
    "    ax.set_title(f\"{strategy}, {total_bits}-bit width\")\n",
    "    ax.set_xticks(start + (num_resources - 1) * (width + resource_gap / 2))\n",
    "    ax.set_xticklabels(reuse_factors, rotation=45)\n",
    "\n",
    "    # if col_idx == 0:\n",
    "    #     ax.set_ylabel(\"Utilization (%)\")\n",
    "\n",
    "    # if row_idx == n_rows - 1:\n",
    "    #     ax.set_xlabel(\"Reuse Factor\")\n",
    "\n",
    "    col_idx += 1\n",
    "    if col_idx == n_cols:\n",
    "        row_idx += 1\n",
    "        col_idx = 0\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "legends = fig.legend(\n",
    "    handles,\n",
    "    labels,\n",
    "    title=\"Resources\",\n",
    "    bbox_to_anchor=[0.3, 1.03],\n",
    "    loc=\"upper left\",\n",
    "    # loc=\"upper right\",\n",
    "    ncol=len(labels) // 2,\n",
    ")\n",
    "\n",
    "xtext = fig.text(0.5, 0.035, \"Reuse Factor\", ha=\"center\", size=18)\n",
    "ytext = fig.text(0.07, 0.5, \"Utilization (%)\", va=\"center\", rotation=\"vertical\", size=18)\n",
    "\n",
    "suptitle = fig.suptitle(\"Pynq-Z2: Resource Utilization Trends\", fontsize=20, y=1.075)\n",
    "\n",
    "plt.subplots_adjust(hspace=0.275, wspace=0.125)\n",
    "plt.show()\n",
    "\n",
    "# fig.savefig(\n",
    "#     \"/mnt/c/Users/Y540/Desktop/pynq_avg_bars.jpg\",\n",
    "#     dpi=300,\n",
    "#     bbox_extra_artists=(legends, xtext, ytext, suptitle),\n",
    "#     bbox_inches=\"tight\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({\"font.size\": 14})\n",
    "\n",
    "grouped = merged_df.xs((\"zcu102\",), level=[\"Board\"]).groupby([\"Precision\", \"Strategy\"])\n",
    "\n",
    "n_groups = len(grouped)\n",
    "n_cols = 2\n",
    "n_rows = 3\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    n_rows, n_cols, dpi=300, figsize=(16, 10), squeeze=False, sharex=True, sharey=False\n",
    ")\n",
    "axes = axes.flatten()\n",
    "\n",
    "width = 0.11\n",
    "colors = [\"#008000\", \"#FF5964\", \"#17BEBB\", \"#FFA500\"]\n",
    "reuse_factors = prediction_df[\"Reuse Factor\"].unique()\n",
    "num_resources = 4\n",
    "resource_gap = 0\n",
    "\n",
    "total_width = num_resources * (2 * width + resource_gap) - resource_gap\n",
    "start = np.arange(1, len(reuse_factors) + 1) - total_width / 2\n",
    "\n",
    "row_idx = 0\n",
    "col_idx = 0\n",
    "for ax, ((precision, strategy), df) in zip(axes, grouped):\n",
    "    for i, (col_gn, col_pred) in enumerate(zip(df.columns[::2], df.columns[1::2])):\n",
    "        gn_vals = df[col_gn]\n",
    "        pred_vals = df[col_pred]\n",
    "\n",
    "        resource_indices = start + i * (2 * width + resource_gap)\n",
    "\n",
    "        for j, reuse_factor in enumerate(reuse_factors):\n",
    "            gn_label = \"\"\n",
    "            pred_label = \"\"\n",
    "            if j == 0:\n",
    "                gn_label = f\"{col_gn}\"\n",
    "                pred_label = f\"{col_pred}\"\n",
    "\n",
    "            ax.bar(\n",
    "                resource_indices[j] - width / 2,\n",
    "                gn_vals[j],\n",
    "                width,\n",
    "                label=gn_label,\n",
    "                color=colors[i % len(colors)],\n",
    "                edgecolor=\"black\",\n",
    "            )\n",
    "            ax.bar(\n",
    "                resource_indices[j] + width / 2,\n",
    "                pred_vals[j],\n",
    "                width,\n",
    "                label=pred_label,\n",
    "                color=colors[i % len(colors)],\n",
    "                edgecolor=\"black\",\n",
    "                hatch=\"///\",\n",
    "            )\n",
    "\n",
    "    total_bits, fraction_bits = fixed_precision_to_bit_width(precision)\n",
    "\n",
    "    ax.set_title(f\"{strategy}, {total_bits}-bit width\")\n",
    "    ax.set_xticks(start + (num_resources - 1) * (width + resource_gap / 2))\n",
    "    ax.set_xticklabels(reuse_factors, rotation=45)\n",
    "\n",
    "    col_idx += 1\n",
    "    if col_idx == n_cols:\n",
    "        row_idx += 1\n",
    "        col_idx = 0\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "legends = fig.legend(\n",
    "    handles,\n",
    "    labels,\n",
    "    title=\"Resources\",\n",
    "    bbox_to_anchor=[0.3, 1.03],\n",
    "    loc=\"upper left\",\n",
    "    ncol=len(labels) // 2,\n",
    ")\n",
    "\n",
    "xtext = fig.text(0.5, 0.035, \"Reuse Factor\", ha=\"center\", size=18)\n",
    "ytext = fig.text(0.07, 0.5, \"Utilization (%)\", va=\"center\", rotation=\"vertical\", size=18)\n",
    "\n",
    "suptitle = fig.suptitle(\"ZCU102: Resource Utilization Trends\", fontsize=20, y=1.075)\n",
    "\n",
    "plt.subplots_adjust(hspace=0.275, wspace=0.125)\n",
    "plt.show()\n",
    "\n",
    "# fig.savefig(\n",
    "#     \"/mnt/c/Users/Y540/Desktop/zcu_avg_bars.jpg\",\n",
    "#     dpi=300,\n",
    "#     bbox_extra_artists=(legends, xtext, ytext, suptitle),\n",
    "#     bbox_inches=\"tight\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gn_grouped_mean = (\n",
    "    benchmark_gn_df.groupby([\"Strategy\", \"Board\", \"Precision\", \"Reuse Factor\"])[[\"CYCLES\"]]\n",
    "    .mean()\n",
    "    .round(0)\n",
    "    .astype(int)\n",
    ")\n",
    "\n",
    "prediction_grouped_mean = (\n",
    "    prediction_df.groupby([\"Strategy\", \"Board\", \"Precision\", \"Reuse Factor\"])[[\"CYCLES\"]]\n",
    "    .mean()\n",
    "    .round(0)\n",
    "    .astype(int)\n",
    ")\n",
    "\n",
    "merged_df = pd.merge(\n",
    "    gn_grouped_mean,\n",
    "    prediction_grouped_mean,\n",
    "    on=(\"Strategy\", \"Board\", \"Precision\", \"Reuse Factor\"),\n",
    "    suffixes=(\" (G)\", \" (P)\"),\n",
    ")\n",
    "\n",
    "merged_df = merged_df[\n",
    "    [\n",
    "        \"CYCLES (G)\",\n",
    "        \"CYCLES (P)\",\n",
    "    ]\n",
    "]\n",
    "merged_df.head()\n",
    "\n",
    "plt.rcParams.update({\"font.size\": 14})\n",
    "\n",
    "grouped = merged_df.groupby([\"Board\", \"Strategy\", \"Precision\"])\n",
    "\n",
    "n_groups = len(grouped)\n",
    "n_cols = 3\n",
    "n_rows = (n_groups // n_cols) + (n_groups % n_cols > 0)\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    n_rows, n_cols, dpi=300, figsize=(16, 10), squeeze=False, sharex=True, sharey=False\n",
    ")\n",
    "axes = axes.flatten()\n",
    "\n",
    "width = 0.35\n",
    "colors = [\"#17BEBB\", \"#FFA500\"]\n",
    "reuse_factors = prediction_df[\"Reuse Factor\"].unique()\n",
    "num_resources = 1\n",
    "resource_gap = 0\n",
    "\n",
    "total_width = num_resources * (2 * width + resource_gap) - resource_gap\n",
    "start = np.arange(1, len(reuse_factors) + 1) - total_width / 2\n",
    "\n",
    "row_idx = 0\n",
    "col_idx = 0\n",
    "for ax, ((board, strategy, precision), df) in zip(axes, grouped):\n",
    "    for i, (col_gn, col_pred) in enumerate(zip(df.columns[::2], df.columns[1::2])):\n",
    "        gn_vals = df[col_gn]\n",
    "        pred_vals = df[col_pred]\n",
    "\n",
    "        resource_indices = start + i * (2 * width + resource_gap)\n",
    "\n",
    "        for j, reuse_factor in enumerate(reuse_factors):\n",
    "            gn_label = \"\"\n",
    "            pred_label = \"\"\n",
    "            if j == 0:\n",
    "                gn_label = f\"{col_gn}\"\n",
    "                pred_label = f\"{col_pred}\"\n",
    "\n",
    "            ax.bar(\n",
    "                resource_indices[j] - width / 2,\n",
    "                gn_vals[j],\n",
    "                width,\n",
    "                label=gn_label,\n",
    "                color=colors[i % len(colors)],\n",
    "                edgecolor=\"black\",\n",
    "            )\n",
    "            ax.bar(\n",
    "                resource_indices[j] + width / 2,\n",
    "                pred_vals[j],\n",
    "                width,\n",
    "                label=pred_label,\n",
    "                color=colors[i % len(colors) + 1],\n",
    "                edgecolor=\"black\",\n",
    "                hatch=\"///\",\n",
    "            )\n",
    "\n",
    "    total_bits, fraction_bits = fixed_precision_to_bit_width(precision)\n",
    "\n",
    "    ax.set_title(f\"{board}, {strategy}, {total_bits}-bit width\")\n",
    "    ax.set_xticks(start + (num_resources - 1) * (width + resource_gap / 2))\n",
    "    ax.set_xticklabels(reuse_factors, rotation=45)\n",
    "\n",
    "    col_idx += 1\n",
    "    if col_idx == n_cols:\n",
    "        row_idx += 1\n",
    "        col_idx = 0\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "legends = fig.legend(handles, labels, bbox_to_anchor=[0.8, 1], loc=\"upper left\")\n",
    "\n",
    "xtext = fig.text(0.5, 0.05, \"Reuse Factor\", ha=\"center\", size=18)\n",
    "ytext = fig.text(0.07, 0.5, \"Cycles\", va=\"center\", rotation=\"vertical\", size=18)\n",
    "\n",
    "fig.suptitle(\"Clock Cycle Trends\", fontsize=20)\n",
    "\n",
    "plt.subplots_adjust(hspace=0.4, wspace=0.2)\n",
    "plt.show()\n",
    "\n",
    "# fig.savefig(\n",
    "#     \"/mnt/c/Users/Y540/Desktop/cycles_avg_bars.jpg\",\n",
    "#     dpi=300,\n",
    "#     bbox_extra_artists=(legends, xtext, ytext),\n",
    "#     bbox_inches=\"tight\",\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

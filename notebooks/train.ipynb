{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e7dce6f",
   "metadata": {},
   "source": [
    "# Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ffaef46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-19 17:53:27.639421: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-01-19 17:53:27.747287: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2026-01-19 17:53:28.618168: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fc793f54d30>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import gc\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None\n",
    "\n",
    "seed_num = 1337\n",
    "np.random.seed(seed_num)\n",
    "keras.utils.set_random_seed(seed_num)\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "torch.manual_seed(seed_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37309ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rule4ml.parsers.data_parser import (\n",
    "    default_board_map,\n",
    "    default_hls4ml_map,\n",
    "    default_layer_type_map,\n",
    "    default_strategy_map,\n",
    "    default_vivado_map,\n",
    "    read_from_json,\n",
    "    json_to_df,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0fd281c",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "data_path = os.path.join(base_path, \"datasets\", \"huggingface\", \"wa-hls4ml\")\n",
    "\n",
    "paths = {\n",
    "    \"train\": os.path.join(data_path, \"train\"),\n",
    "    \"val\": os.path.join(data_path, \"val\"),\n",
    "    \"test\": os.path.join(data_path, \"test\"),\n",
    "    \"exemplar\": os.path.join(data_path, \"exemplar\"),\n",
    "}\n",
    "\n",
    "global_categorical_maps = {\n",
    "    \"strategy\": default_strategy_map,\n",
    "    \"board\": default_board_map,\n",
    "    \"hls4ml_version\": default_hls4ml_map,\n",
    "    \"vivado_version\": default_vivado_map,\n",
    "}\n",
    "sequential_categorical_maps = {\n",
    "    \"layer_type\": default_layer_type_map,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823cd49b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 147192 records from test set.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hamza/Projects/rule4ml/rule4ml/parsers/data_parser.py:922: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat(dataframes, axis=0).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 887 records from exemplar set.\n"
     ]
    }
   ],
   "source": [
    "dataframes = {}\n",
    "for key, path in paths.items():\n",
    "    if os.path.exists(os.path.join(path, \"data.feather\")):\n",
    "        df = pd.read_feather(os.path.join(path, \"data.feather\"))\n",
    "        df[\"sequential_inputs\"] = df[\"sequential_inputs\"].apply(json.loads)\n",
    "        df[\"sequential_inputs\"] = df[\"sequential_inputs\"].apply(\n",
    "            lambda x: pd.DataFrame(x) if isinstance(x, list) else list(x)\n",
    "        )\n",
    "        dataframes[key] = df.dropna(subset=[\"bram\", \"dsp\", \"ff\", \"lut\", \"cycles\", \"interval\"])\n",
    "    else:\n",
    "        repeat = 5 if key in [\"train\", \"val\"] else 1\n",
    "        json_data = read_from_json(\n",
    "            [\n",
    "                os.path.join(path, \"*2_20_merged.json\"),\n",
    "                os.path.join(path, \"*2layer_merged.json\"),\n",
    "                os.path.join(path, \"*3layer_merged.json\"),\n",
    "                os.path.join(path, \"*latency_merged.json\"),\n",
    "                os.path.join(path, \"*resource_merged.json\"),\n",
    "                os.path.join(path, \"*exemplar_models.json\"),\n",
    "            ]\n",
    "            + [\n",
    "                os.path.join(path, \"*conv1d_merged.json\"),\n",
    "                os.path.join(path, \"*conv2d_merged.json\")\n",
    "            ] * repeat,\n",
    "            batch_size=128,\n",
    "            max_workers=16\n",
    "        )\n",
    "        if len(json_data):\n",
    "            print(f\"Loaded {len(json_data)} records from {key} set.\")\n",
    "        else:\n",
    "            print(f\"No data found for {key}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        normalize = key in [\"train\", \"val\"]\n",
    "        df = json_to_df(\n",
    "            json_data,\n",
    "            global_categorical_maps,\n",
    "            sequential_categorical_maps,\n",
    "            normalize=normalize,\n",
    "            max_workers=16\n",
    "        )\n",
    "        if not df.empty:\n",
    "            df_to_save = df\n",
    "            df_to_save[\"sequential_inputs\"] = df_to_save[\"sequential_inputs\"].apply(\n",
    "                lambda x: x.to_dict(orient=\"records\") if isinstance(x, pd.DataFrame) else x\n",
    "            )\n",
    "            df_to_save[\"sequential_inputs\"] = df_to_save[\"sequential_inputs\"].apply(json.dumps)\n",
    "            df_to_save.to_feather(os.path.join(paths[key], \"data.feather\"))\n",
    "\n",
    "            # revert back to original format after saving\n",
    "            df_to_save[\"sequential_inputs\"] = df_to_save[\"sequential_inputs\"].apply(json.loads)\n",
    "            df_to_save[\"sequential_inputs\"] = df_to_save[\"sequential_inputs\"].apply(\n",
    "                lambda x: pd.DataFrame(x) if isinstance(x, list) else list(x)\n",
    "            )\n",
    "\n",
    "        dataframes[key] = df.dropna(subset=[\"bram\", \"dsp\", \"ff\", \"lut\", \"cycles\", \"interval\"])\n",
    "        del json_data\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad7c5537",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_feature_labels = [\n",
    "    \"strategy\",\n",
    "    \"board\",\n",
    "    \"hls4ml_version\",\n",
    "    \"vivado_version\",\n",
    "    # \"clock_period\",\n",
    "    \"bit_width\",\n",
    "    \"reuse_mean\",\n",
    "    \"dense_inputs_mean\",\n",
    "    \"dense_outputs_mean\",\n",
    "    \"dense_parameters_mean\",\n",
    "    \"dense_reuse_mean\",\n",
    "    \"dense_count\",\n",
    "    \"conv1d_inputs_mean\",\n",
    "    \"conv1d_outputs_mean\",\n",
    "    \"conv1d_parameters_mean\",\n",
    "    \"conv1d_filters_mean\",\n",
    "    \"conv1d_kernel_size_mean\",\n",
    "    \"conv1d_strides_mean\",\n",
    "    \"conv1d_reuse_mean\",\n",
    "    \"conv1d_count\",\n",
    "    \"conv2d_inputs_mean\",\n",
    "    \"conv2d_outputs_mean\",\n",
    "    \"conv2d_parameters_mean\",\n",
    "    \"conv2d_filters_mean\",\n",
    "    \"conv2d_kernel_size_mean\",\n",
    "    \"conv2d_strides_mean\",\n",
    "    \"conv2d_reuse_mean\",\n",
    "    \"conv2d_count\",\n",
    "    \"batchnormalization_inputs_mean\",\n",
    "    \"batchnormalization_outputs_mean\",\n",
    "    \"batchnormalization_parameters_mean\",\n",
    "    \"batchnormalization_count\",\n",
    "    \"add_count\",\n",
    "    \"concatenate_count\",\n",
    "    \"dropout_count\",\n",
    "    \"relu_count\",\n",
    "    \"sigmoid_count\",\n",
    "    \"tanh_count\",\n",
    "    \"softmax_inputs_mean\",\n",
    "    \"softmax_outputs_mean\",\n",
    "    \"softmax_count\",\n",
    "    \"total_add\",\n",
    "    \"total_mult\",\n",
    "    \"total_lookup\",\n",
    "    \"total_logical\"\n",
    "]\n",
    "sequential_feature_labels = [\n",
    "    \"layer_type\",\n",
    "    \"layer_input_size\",\n",
    "    \"layer_output_size\",\n",
    "    \"layer_parameter_count\",\n",
    "    \"layer_trainable_parameter_count\",\n",
    "    \"layer_filters\",\n",
    "    \"layer_kernel_height\",\n",
    "    \"layer_kernel_width\",\n",
    "    \"layer_stride_height\",\n",
    "    \"layer_stride_width\",\n",
    "    \"layer_reuse\",\n",
    "    \"layer_op_add\",\n",
    "    \"layer_op_mult\",\n",
    "    \"layer_op_lookup\",\n",
    "    \"layer_op_logical\"\n",
    "]\n",
    "target_columns = [\"bram\", \"dsp\", \"ff\", \"lut\", \"cycles\", \"interval\"]\n",
    "\n",
    "inputs_dfs = {}\n",
    "targets_dfs = {}\n",
    "for key, df in dataframes.items():\n",
    "    inputs_dfs[key] = df[global_feature_labels].copy()\n",
    "    if len(sequential_feature_labels) > 0:\n",
    "        inputs_dfs[key][\"sequential_inputs\"] = df[\"sequential_inputs\"].apply(\n",
    "            lambda x: x[sequential_feature_labels]\n",
    "        )\n",
    "    targets_dfs[key] = df[target_columns].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c6e4cd",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20a6748",
   "metadata": {},
   "source": [
    "### MLP specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fe18d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rule4ml.models.architectures import (\n",
    "    MLPSettings,\n",
    "    TorchMLP\n",
    ")\n",
    "from rule4ml.models.wrappers import TorchModelWrapper\n",
    "from rule4ml.models.utils import torch_weights_init\n",
    "\n",
    "target_labels = [\"dsp\"]\n",
    "\n",
    "global_input_shape = (None, len(global_feature_labels))\n",
    "output_shape = (None, len(target_labels))\n",
    "\n",
    "mlp_settings = MLPSettings(\n",
    "    embedding_layers=[16 for _ in range(len(global_categorical_maps))],\n",
    "    numerical_dense_layers=[64],\n",
    "    dense_layers=[128, 64, 32],\n",
    "    dense_dropouts=[0.2, 0.2, 0.1],\n",
    ")\n",
    "\n",
    "cuda_available = False\n",
    "device = \"cuda\" if cuda_available else \"cpu\"\n",
    "if cuda_available:\n",
    "    print(\"Using GPU for training.\")\n",
    "    torch.cuda.manual_seed(seed_num)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "else:\n",
    "    print(\"Using CPU for training.\")\n",
    "\n",
    "import torch  # making sure torch is installed\n",
    "torch_model = TorchMLP(\n",
    "    settings=mlp_settings,\n",
    "    input_shape=global_input_shape,\n",
    "    output_shape=output_shape,\n",
    "    categorical_maps=global_categorical_maps,\n",
    "    name=f\"{'-'.join([x.upper() for x in target_labels])}_MLP\",\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "torch_wrapper = TorchModelWrapper()\n",
    "torch_wrapper.set_model(torch_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faaa9a26",
   "metadata": {},
   "source": [
    "### GNN specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88a0d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rule4ml.models.architectures import (\n",
    "    GNNSettings,\n",
    "    TorchGNN,\n",
    ")\n",
    "from rule4ml.models.wrappers import (\n",
    "    TorchModelWrapper,\n",
    ")\n",
    "from rule4ml.models.utils import torch_weights_init\n",
    "\n",
    "target_labels = [\"dsp\"]\n",
    "\n",
    "global_input_shape = (None, len(global_feature_labels))\n",
    "sequential_input_shape = (None, len(sequential_feature_labels))\n",
    "output_shape = (None, len(target_labels))\n",
    "\n",
    "network_settings = {\n",
    "    \"bram\": GNNSettings(\n",
    "        global_embedding_layers=[8, 32, 16, 8],\n",
    "        seq_embedding_layers=[8],\n",
    "        numerical_dense_layers=[32, 32],\n",
    "        gconv_layers=[64, 128, 16, 16],\n",
    "        dense_layers=[32, 16],\n",
    "        dense_dropouts=[0.1, 0.0],\n",
    "    ),\n",
    "    \"dsp\": GNNSettings(\n",
    "        global_embedding_layers=[32, 16, 8, 32],\n",
    "        seq_embedding_layers=[8],\n",
    "        numerical_dense_layers=[32, 16],\n",
    "        gconv_layers=[128, 32, 32],\n",
    "        dense_layers=[32, 64, 32, 128, 128],\n",
    "        dense_dropouts=[0.0, 0.0, 0.1, 0.0],\n",
    "    ),\n",
    "    \"ff\": GNNSettings(\n",
    "        global_embedding_layers=[16, 8, 8, 16],\n",
    "        seq_embedding_layers=[32],\n",
    "        numerical_dense_layers=[32, 16],\n",
    "        gconv_layers=[128, 16],\n",
    "        dense_layers=[16, 64],\n",
    "        dense_dropouts=[0.2, 0.2],\n",
    "    ),\n",
    "    \"lut\": GNNSettings(\n",
    "        global_embedding_layers=[16, 32, 32, 32],\n",
    "        seq_embedding_layers=[16],\n",
    "        numerical_dense_layers=[],\n",
    "        gconv_layers=[128, 64],\n",
    "        dense_layers=[128, 128, 16, 128],\n",
    "        bayesian_dense=[False, False, False, False],\n",
    "        dense_dropouts=[0.1, 0.2, 0.0, 0.3],\n",
    "        bayesian_output=False,\n",
    "    )\n",
    "}\n",
    "gnn_settings = network_settings[\"_\".join(target_labels)]\n",
    "\n",
    "cuda_available = False\n",
    "device = \"cuda\" if cuda_available else \"cpu\"\n",
    "if cuda_available:\n",
    "    print(\"Using GPU for training.\")\n",
    "    torch.cuda.manual_seed(seed_num)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "else:\n",
    "    print(\"Using CPU for training.\")\n",
    "\n",
    "import torch  # making sure torch is installed\n",
    "torch_model = TorchGNN(\n",
    "    settings=gnn_settings,\n",
    "    global_input_shape=global_input_shape,\n",
    "    sequential_input_shape=sequential_input_shape,\n",
    "    output_shape=output_shape,\n",
    "    global_categorical_maps=global_categorical_maps,\n",
    "    sequential_categorical_maps=sequential_categorical_maps,\n",
    "    name=f\"{'-'.join([x.upper() for x in target_labels])}_GNN\",\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "torch_wrapper = TorchModelWrapper()\n",
    "torch_wrapper.set_model(torch_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3754931",
   "metadata": {},
   "source": [
    "### Common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c296fd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rule4ml.models.callbacks import (\n",
    "    EarlyStopping,\n",
    "    ModelCheckpoint,\n",
    "    ReduceLROnPlateau\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        mode=\"min\",\n",
    "        patience=20,\n",
    "        min_delta=0.0,\n",
    "        restore_best=True\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        dirpath=os.path.join(base_path, \"notebooks\", \"models\"),\n",
    "        monitor=\"val_loss\",\n",
    "        mode=\"min\"\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\",\n",
    "        mode=\"min\",\n",
    "        factor=0.5,\n",
    "        patience=10,\n",
    "        min_delta=0.0,\n",
    "        min_lr=1e-6\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d669e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rule4ml.models.wrappers import TrainSettings\n",
    "from rule4ml.models.metrics import TorchParametricSMAPE, TorchParametricR2\n",
    "from rule4ml.models.utils import get_optimizer_from_str\n",
    "\n",
    "scaler = None\n",
    "metrics = [\n",
    "    TorchParametricSMAPE(idx, name=f\"smape_{target_labels[idx]}\", eps=1e-6, scaler=scaler, device=device)\n",
    "    for idx in range(len(target_labels))\n",
    "]\n",
    "metrics += [\n",
    "    TorchParametricR2(idx, name=f\"r2_{target_labels[idx]}\", eps=1e-6, scaler=scaler, device=device)\n",
    "    for idx in range(len(target_labels))\n",
    "]\n",
    "\n",
    "global_settings = {\n",
    "    \"bram\": TrainSettings(\n",
    "        num_epochs=50,\n",
    "        batch_size=32,\n",
    "        learning_rate=1e-4,\n",
    "        loss_function=\"msle\",\n",
    "        optimizer=\"adam\",\n",
    "        metrics=metrics,\n",
    "    ),\n",
    "    \"dsp\": TrainSettings(\n",
    "        num_epochs=50,\n",
    "        batch_size=64,\n",
    "        learning_rate=1e-4,\n",
    "        loss_function=\"msle\",\n",
    "        optimizer=\"adam\",\n",
    "        metrics=metrics,\n",
    "    ),\n",
    "    \"ff\": TrainSettings(\n",
    "        num_epochs=50,\n",
    "        batch_size=64,\n",
    "        learning_rate=1e-4,\n",
    "        loss_function=\"msle\",\n",
    "        optimizer=\"adam\",\n",
    "        metrics=metrics,\n",
    "    ),\n",
    "    \"lut\": TrainSettings(\n",
    "        num_epochs=50,\n",
    "        batch_size=32,\n",
    "        learning_rate=1e-4,\n",
    "        loss_function=\"msle\",\n",
    "        optimizer=\"adam\",\n",
    "        metrics=metrics,\n",
    "    )\n",
    "}\n",
    "train_settings = global_settings[\"_\".join(target_labels)]\n",
    "\n",
    "torch_wrapper.build_dataset(\n",
    "    inputs_dfs[\"train\"],\n",
    "    targets_dfs[\"train\"][target_labels],\n",
    "    train_settings.batch_size,\n",
    "    val_inputs_df=inputs_dfs[\"val\"],\n",
    "    val_targets_df=targets_dfs[\"val\"][target_labels],\n",
    "    train_repeats=1,\n",
    "    shuffle=True,\n",
    "    scaler=scaler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4497659b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_history = torch_wrapper.fit(\n",
    "    train_settings,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75ef72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_fit_history(fit_history, metric_name):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(fit_history[\"train\"][metric_name], label=\"Train\")\n",
    "    if \"val\" in fit_history:\n",
    "        plt.plot(fit_history[\"val\"][metric_name], label=\"Validation\")\n",
    "    plt.title(f\"Model {metric_name} history\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(metric_name)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_fit_history(fit_history, \"loss\")\n",
    "plot_fit_history(fit_history, f\"smape_{target_labels[0]}\")\n",
    "plot_fit_history(fit_history, f\"r2_{target_labels[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e63bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Train Loss: {fit_history[\"train\"][\"loss\"][-1]}')\n",
    "print(f'Train SMAPE: {fit_history[\"train\"][f\"smape_{target_labels[0]}\"][-1]}')\n",
    "print(f'Train R2: {fit_history[\"train\"][f\"r2_{target_labels[0]}\"][-1]}')\n",
    "\n",
    "print(\"=\"*40)\n",
    "\n",
    "print(f'Val Loss: {fit_history[\"val\"][\"loss\"][-1]}')\n",
    "print(f'Val SMAPE: {fit_history[\"val\"][f\"smape_{target_labels[0]}\"][-1]}')\n",
    "print(f'Val R2: {fit_history[\"val\"][f\"r2_{target_labels[0]}\"][-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b0de33",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_wrapper.save(\"./models\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env (3.10.16)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
